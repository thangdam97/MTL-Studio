#!/usr/bin/env python3
"""
Comprehensive JP Grammar Pattern Corpus Scanner
Scans 148 EPUBs to find high-frequency grammar patterns NOT in the current RAG database.
"""
import os, re, json, zipfile, sys
from collections import Counter, defaultdict
from html.parser import HTMLParser

# === HTML text extractor ===
class HTMLTextExtractor(HTMLParser):
    def __init__(self):
        super().__init__()
        self.result = []
        self.skip = False
    def handle_starttag(self, tag, attrs):
        if tag in ('script', 'style', 'rt'):  # skip ruby annotation
            self.skip = True
    def handle_endtag(self, tag):
        if tag in ('script', 'style', 'rt'):
            self.skip = False
    def handle_data(self, data):
        if not self.skip:
            self.result.append(data)
    def get_text(self):
        return ''.join(self.result)

def extract_text_from_epub(epub_path):
    """Extract all Japanese text from an EPUB file."""
    text_parts = []
    try:
        with zipfile.ZipFile(epub_path, 'r') as z:
            xhtml_files = [f for f in z.namelist() 
                          if f.endswith(('.xhtml', '.html', '.htm'))
                          and 'nav' not in f.lower() 
                          and 'toc' not in f.lower()]
            for xf in sorted(xhtml_files):
                try:
                    with z.open(xf) as f:
                        content = f.read().decode('utf-8', errors='ignore')
                        parser = HTMLTextExtractor()
                        parser.feed(content)
                        text = parser.get_text().strip()
                        if text and len(text) > 50:
                            text_parts.append(text)
                except:
                    continue
    except Exception as e:
        print(f"  âš ï¸ Failed to read: {os.path.basename(epub_path)}: {e}", file=sys.stderr)
    return '\n'.join(text_parts)


# === Define CANDIDATE patterns to scan for ===
CANDIDATE_PATTERNS = {
    # --- HEDGING / UNCERTAINTY (detector has rules, RAG has NOTHING) ---
    "hedging_kamoshirenai": (r'ã‹ã‚‚ã—ã‚Œãªã„|ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“|ã‹ã‚‚çŸ¥ã‚Œãªã„', "ã‹ã‚‚ã—ã‚Œãªã„ â€” may/might"),
    "hedging_darou": (r'ã ã‚ã†|ã§ã—ã‚‡ã†|ã ã‚ã†ã‹', "ã ã‚ã†/ã§ã—ã‚‡ã† â€” probably/I suppose"),
    "hedging_rashii": (r'ã‚‰ã—ã„[ã€‚ã€ã€]|ã£ã½ã„[ã€‚ã€ã€]', "ã‚‰ã—ã„/ã£ã½ã„ â€” seems like/apparently"),
    "hedging_mitai": (r'ã¿ãŸã„ã |ã¿ãŸã„ãª|ã¿ãŸã„ã«', "ã¿ãŸã„ â€” like/seems"),
    "hedging_you_da": (r'ã‚ˆã†ã [ã€‚ã€]|ã‚ˆã†ãª[ã€‚ã€]|ã‚ˆã†ã«[ã€‚ã€]', "ã‚ˆã†ã  â€” it appears/as if"),
    "hedging_to_omou": (r'ã¨æ€ã†|ã¨æ€ã£ãŸ|ã¨æ€ã„ã¾ã™', "ã¨æ€ã† â€” I think/feel"),
    "hedging_ki_ga_suru": (r'æ°—ãŒã™ã‚‹|æ°—ãŒã—ãŸ', "æ°—ãŒã™ã‚‹ â€” I get the feeling"),

    # --- RESPONSE PARTICLES (detector has rules, RAG has NOTHING) ---
    "response_aa": (r'ã€Œã‚ã‚[ã€ã€‚ã€]|ã€Œã‚ã[ã€ã€‚ã€]', "ã‚ã‚ â€” response particle"),
    "response_un": (r'ã€Œã†ã‚“[ã€ã€‚ã€]|ã€Œã†ãƒ¼ã‚“[ã€ã€‚ã€]', "ã†ã‚“ â€” casual yes"),
    "response_ee": (r'ã€Œãˆãˆ[ã€ã€‚ã€]', "ãˆãˆ â€” polite yes"),
    "response_sou": (r'ã€Œãã†[ã ã‹ãªã­]', "ãã†ã ã­/ãã†ã‹ â€” I see/is that so"),
    "response_hee": (r'ã€Œã¸ãˆ[ãƒ¼ï½]?[ã€ã€‚ã€]', "ã¸ãˆ â€” surprised interest"),
    "response_oi": (r'ã€ŒãŠã„[ã€ã€‚ã€ï¼]', "ãŠã„ â€” hey (masculine)"),
    "response_nee": (r'ã€Œã­ãˆ[ã€ã€‚ã€]|ã€Œã­ã‡[ã€ã€‚ã€]', "ã­ãˆ â€” hey (attention-getting)"),
    "response_hora": (r'ã»ã‚‰[ã€ã€‚]|ã»ã‚‰[ã­]?[ã€ã€‚ã€]', "ã»ã‚‰ â€” look/see"),

    # --- NATURAL TRANSITIONS (detector has rules, RAG has NOTHING) ---
    "transition_tonikaku": (r'ã¨ã«ã‹ã', "ã¨ã«ã‹ã â€” anyway/in any case"),
    "transition_tokoro_de": (r'ã¨ã“ã‚ã§', "ã¨ã“ã‚ã§ â€” by the way"),
    "transition_sate": (r'ã•ã¦[ã€ã€‚]|ã•ã¦ã¨', "ã•ã¦ â€” well then/now"),
    "transition_soredewa": (r'ãã‚Œã§ã¯|ãã‚Œã˜ã‚ƒ|ã˜ã‚ƒã‚', "ãã‚Œã§ã¯/ã˜ã‚ƒã‚ â€” well then"),
    "transition_tsumari": (r'ã¤ã¾ã‚Š', "ã¤ã¾ã‚Š â€” in other words/basically"),
    "transition_dakara": (r'ã ã‹ã‚‰[ã€ã€‚]|ã§ã™ã‹ã‚‰', "ã ã‹ã‚‰ â€” so/that's why"),
    "transition_shikashi": (r'ã—ã‹ã—[ã€ã€‚]', "ã—ã‹ã— â€” however"),
    "transition_demo": (r'ã§ã‚‚[ã€ã€‚]', "ã§ã‚‚ â€” but/however"),
    "transition_soreni": (r'ãã‚Œã«[ã€ã€‚]', "ãã‚Œã« â€” besides/moreover"),
    "transition_datte_reason": (r'ã ã£ã¦[ã€ã€‚ã€]', "ã ã£ã¦ â€” because/but (reason)"),
    "transition_sorede": (r'ãã‚Œã§[ã€ã€‚ï¼Ÿ]', "ãã‚Œã§ â€” and then/so"),
    "transition_sokode": (r'ãã“ã§[ã€ã€‚]', "ãã“ã§ â€” thereupon"),

    # --- ONOMATOPOEIA / MIMETIC WORDS (not in detector OR RAG) ---
    "ono_dokidoki": (r'ãƒ‰ã‚­ãƒ‰ã‚­|ã©ãã©ã|ãƒ‰ã‚­ãƒƒ|ã©ãã£', "ãƒ‰ã‚­ãƒ‰ã‚­ â€” heart pounding"),
    "ono_kirakira": (r'ã‚­ãƒ©ã‚­ãƒ©|ãã‚‰ãã‚‰', "ã‚­ãƒ©ã‚­ãƒ© â€” sparkling"),
    "ono_iraira": (r'ã‚¤ãƒ©ã‚¤ãƒ©|ã„ã‚‰ã„ã‚‰', "ã‚¤ãƒ©ã‚¤ãƒ© â€” irritated"),
    "ono_wakuwaku": (r'ãƒ¯ã‚¯ãƒ¯ã‚¯|ã‚ãã‚ã', "ãƒ¯ã‚¯ãƒ¯ã‚¯ â€” excited"),
    "ono_nikoniko": (r'ãƒ‹ã‚³ãƒ‹ã‚³|ã«ã“ã«ã“|ãƒ‹ã‚³ãƒƒ|ã«ã“ã£', "ãƒ‹ã‚³ãƒ‹ã‚³ â€” smiling"),
    "ono_gakkari": (r'ãŒã£ã‹ã‚Š|ã‚¬ãƒƒã‚«ãƒª', "ãŒã£ã‹ã‚Š â€” disappointed"),
    "ono_bikkuri": (r'ã³ã£ãã‚Š|ãƒ“ãƒƒã‚¯ãƒª', "ã³ã£ãã‚Š â€” startled"),
    "ono_niyari": (r'ãƒ‹ãƒ¤ãƒª|ã«ã‚„ã‚Š|ãƒ‹ãƒ¤ãƒ‹ãƒ¤|ã«ã‚„ã«ã‚„|ãƒ‹ãƒ¤ãƒƒ', "ãƒ‹ãƒ¤ãƒª â€” smirk/grin"),
    "ono_jirojiro": (r'ã‚¸ãƒ­ã‚¸ãƒ­|ã˜ã‚ã˜ã‚|ã‚¸ãƒ¼ãƒƒ|ã˜ãƒ¼ã£|ã‚¸ãƒ­ãƒƒ', "ã‚¸ãƒ­ã‚¸ãƒ­ â€” staring"),
    "ono_hakkiri": (r'ã¯ã£ãã‚Š|ãƒãƒƒã‚­ãƒª', "ã¯ã£ãã‚Š â€” clearly"),
    "ono_pittari": (r'ã´ã£ãŸã‚Š|ãƒ”ãƒƒã‚¿ãƒª', "ã´ã£ãŸã‚Š â€” perfectly"),
    "ono_sowasowa": (r'ã‚½ãƒ¯ã‚½ãƒ¯|ãã‚ãã‚', "ã‚½ãƒ¯ã‚½ãƒ¯ â€” restless"),
    "ono_mojimuji": (r'ãƒ¢ã‚¸ãƒ¢ã‚¸|ã‚‚ã˜ã‚‚ã˜', "ãƒ¢ã‚¸ãƒ¢ã‚¸ â€” fidgeting shyly"),
    "ono_boyatto": (r'ã¼ãƒ¼ã£ã¨|ãƒœãƒ¼ãƒƒã¨|ã¼ã†ã£ã¨', "ã¼ãƒ¼ã£ã¨ â€” spacing out"),
    "ono_uttori": (r'ã†ã£ã¨ã‚Š|ã‚¦ãƒƒãƒˆãƒª', "ã†ã£ã¨ã‚Š â€” entranced"),
    "ono_shikkari": (r'ã—ã£ã‹ã‚Š|ã‚·ãƒƒã‚«ãƒª', "ã—ã£ã‹ã‚Š â€” firmly"),
    "ono_gussuri": (r'ãã£ã™ã‚Š|ã‚°ãƒƒã‚¹ãƒª', "ãã£ã™ã‚Š â€” sound asleep"),
    "ono_kurukuru": (r'ã‚¯ãƒ«ã‚¯ãƒ«|ãã‚‹ãã‚‹', "ã‚¯ãƒ«ã‚¯ãƒ« â€” spinning"),
    "ono_chira": (r'ãƒãƒ©[ãƒƒãƒãƒª]|ã¡ã‚‰[ã£ã¡ã‚Š]', "ãƒãƒ©ãƒƒ â€” glance"),
    "ono_gu": (r'ã‚°ãƒƒ[ã¨]|ãã£[ã¨]', "ã‚°ãƒƒã¨ â€” tightly/firmly"),
    "ono_bata_bata": (r'ãƒã‚¿ãƒã‚¿|ã°ãŸã°ãŸ', "ãƒã‚¿ãƒã‚¿ â€” bustling"),
    "ono_potsu_potsu": (r'ãƒãƒ„ãƒãƒ„|ã½ã¤ã½ã¤|ãƒãƒ„ãƒª|ã½ã¤ã‚Š', "ãƒãƒ„ãƒª â€” muttering"),
    "ono_suya_suya": (r'ã‚¹ãƒ¤ã‚¹ãƒ¤|ã™ã‚„ã™ã‚„', "ã‚¹ãƒ¤ã‚¹ãƒ¤ â€” sleeping peacefully"),
    "ono_mota_mota": (r'ãƒ¢ã‚¿ãƒ¢ã‚¿|ã‚‚ãŸã‚‚ãŸ', "ãƒ¢ã‚¿ãƒ¢ã‚¿ â€” dawdling"),
    "ono_pero_pero": (r'ãƒšãƒ­ãƒšãƒ­|ãºã‚ãºã‚|ãƒšãƒ­ãƒƒ|ãºã‚ã£', "ãƒšãƒ­ â€” lick"),
    "ono_gata_gata": (r'ã‚¬ã‚¿ã‚¬ã‚¿|ãŒãŸãŒãŸ|ã‚¬ã‚¿ãƒƒ', "ã‚¬ã‚¿ â€” rattling/shaking"),
    "ono_zawa_zawa": (r'ã‚¶ãƒ¯ã‚¶ãƒ¯|ã–ã‚ã–ã‚|ã‚¶ãƒ¯ãƒƒ', "ã‚¶ãƒ¯ â€” murmuring crowd"),
    "ono_hara_hara": (r'ãƒãƒ©ãƒãƒ©|ã¯ã‚‰ã¯ã‚‰', "ãƒãƒ©ãƒãƒ© â€” anxious/tears falling"),
    "ono_fuwa_fuwa": (r'ãƒ•ãƒ¯ãƒ•ãƒ¯|ãµã‚ãµã‚|ãƒ•ãƒ¯ãƒƒ', "ãƒ•ãƒ¯ â€” fluffy/floating"),
    "ono_kya": (r'ã‚­ãƒ£ãƒƒ|ãã‚ƒã£|ãã‚ƒã‚|ã‚­ãƒ£ãƒ¼', "ã‚­ãƒ£ãƒ¼ â€” shriek/squeal"),

    # --- KEIGO / POLITENESS SHIFTS (not in RAG) ---
    "keigo_desu_masu": (r'ã§ã™[ã€‚ã€ã€]|ã¾ã™[ã€‚ã€ã€]|ã¾ã—ãŸ[ã€‚ã€ã€]|ã§ã—ãŸ[ã€‚ã€ã€]', "ã§ã™/ã¾ã™ â€” polite form"),
    "keigo_kudasai": (r'ãã ã•ã„|ä¸‹ã•ã„', "ãã ã•ã„ â€” please"),
    "keigo_gozaimasu": (r'ã”ã–ã„ã¾ã™|ã”ã–ã„ã¾ã—ãŸ', "ã”ã–ã„ã¾ã™ â€” very polite"),
    "keigo_itadaku": (r'ã„ãŸã [ããã‘]|é ‚[ããã‘]', "ã„ãŸã ã â€” humble receive"),
    "keigo_ossharu": (r'ãŠã£ã—ã‚ƒ[ã„ã‚‹ã‚Œã£]', "ãŠã£ã—ã‚ƒã‚‹ â€” honorific say"),
    "keigo_irassharu": (r'ã„ã‚‰ã£ã—ã‚ƒ[ã„ã‚‹ã‚Œã£]', "ã„ã‚‰ã£ã—ã‚ƒã‚‹ â€” honorific be/go"),

    # --- INNER MONOLOGUE / NARRATION (common in LN, not in RAG) ---
    "mono_omowazu": (r'æ€ã‚ãš', "æ€ã‚ãš â€” involuntarily"),
    "mono_tsui": (r'ã¤ã„[ã€ã€‚]|ã¤ã„ã¤ã„', "ã¤ã„ â€” inadvertently"),
    "mono_futo": (r'ãµã¨[ã€ã€‚]|ãµã£ã¨[ã€ã€‚]', "ãµã¨ â€” suddenly (thought)"),
    "mono_naze_ka": (r'ãªãœã‹|ä½•æ•…ã‹', "ãªãœã‹ â€” for some reason"),
    "mono_douyara": (r'ã©ã†ã‚„ã‚‰', "ã©ã†ã‚„ã‚‰ â€” it seems"),
    "mono_masani": (r'ã¾ã•ã«', "ã¾ã•ã« â€” exactly/precisely"),
    "mono_sore_demo": (r'ãã‚Œã§ã‚‚', "ãã‚Œã§ã‚‚ â€” even so/still"),
    "mono_soushite": (r'ãã†ã—ã¦|ãã—ã¦', "ãã—ã¦ â€” and then"),
    "mono_ikanimo": (r'ã„ã‹ã«ã‚‚', "ã„ã‹ã«ã‚‚ â€” indeed/truly"),
    "mono_doushitemo": (r'ã©ã†ã—ã¦ã‚‚', "ã©ã†ã—ã¦ã‚‚ â€” no matter what"),

    # --- SENTENCE-ENDING NUANCES (expand what RAG has) ---
    "ending_kana": (r'ã‹ãª[ã€‚ã€]|ã‹ãªã[ã€‚ã€]', "ã‹ãª â€” I wonder"),
    "ending_kashira": (r'ã‹ã—ã‚‰[ã€‚ã€]', "ã‹ã—ã‚‰ â€” I wonder (feminine)"),
    "ending_noda": (r'ã®ã [ã€‚ã€]|ã‚“ã [ã€‚ã€]', "ã®ã /ã‚“ã  â€” explanatory"),
    "ending_desho": (r'ã§ã—ã‚‡[ã€‚ï¼Ÿã€ã†]', "ã§ã—ã‚‡ â€” right?/probably"),
    "ending_tte": (r'ã£ã¦[ã€‚ã€]|ã£ã¦ã°[ã€‚ã€ï¼]', "ã£ã¦/ã£ã¦ã° â€” I said/telling you"),
    "ending_mono": (r'ã‚‚ã®[ã€‚ã€]|ã‚‚ã‚“[ã€‚ã€]', "ã‚‚ã®/ã‚‚ã‚“ â€” because (childish)"),
    "ending_nano": (r'ãªã®[ã€‚ï¼Ÿã€]', "ãªã® â€” is it?/it is (soft)"),
    "ending_sa": (r'ã•[ã€‚ã€]', "ã• â€” casual masculine assertion"),
    "ending_wa_yo": (r'ã‚ã‚ˆ[ã€‚ã€ï¼]|ã®ã‚ˆ[ã€‚ã€ï¼]', "ã‚ã‚ˆ/ã®ã‚ˆ â€” feminine emphasis"),
    "ending_zo": (r'ã[ã€‚ã€ï¼]', "ã â€” masculine emphasis"),
    "ending_na_excl": (r'ãª[ã€‚ã€ï¼]', "ãª â€” exclamatory/reflective"),

    # --- CAUSATIVE / GIVING-RECEIVING (complex grammar, not in RAG) ---
    "causative_saseru": (r'ã•ã›[ã‚‹ãŸã¦ã‚‰]', "ã•ã›ã‚‹ â€” make/let do"),
    "receiving_morau": (r'ã‚‚ã‚‰ã†|ã‚‚ã‚‰ã£ãŸ|ã‚‚ã‚‰ãˆ[ã‚‹ã°]', "ã‚‚ã‚‰ã† â€” get someone to"),
    "receiving_kureru": (r'ãã‚Œã‚‹|ãã‚ŒãŸ|ãã‚Œãªã„', "ãã‚Œã‚‹ â€” do for me (favor)"),
    "receiving_ageru": (r'ã¦ã‚ã’ã‚‹|ã¦ã‚ã’ãŸ', "ã¦ã‚ã’ã‚‹ â€” do for someone"),

    # --- QUOTATION / HEARSAY ---
    "quote_tte_iu": (r'ã£ã¦è¨€[ã†ã£ãŸ]|ã¨è¨€[ã†ã£ãŸ]', "ã£ã¦è¨€ã† â€” said/called"),
    "quote_to_iu": (r'ã¨ã„ã†[ã®ã“ã‚‚ã¯]', "ã¨ã„ã† â€” so-called/the thing called"),
    "quote_sou_da_hearsay": (r'ã ãã†ã |ã ãã†ã§ã™|ã¨ã®ã“ã¨ã ', "ãã†ã  â€” I heard/reportedly"),
    "quote_toka_iu": (r'ã¨ã‹[è¨€ã„]', "ã¨ã‹è¨€ã† â€” something like"),

    # --- DESIRE / INTENTION ---
    "desire_tai": (r'ãŸã„[ã€‚ã€ã€]|ãŸã‹ã£ãŸ[ã€‚ã€ã€]|ãŸããª[ã„ã‹]', "ãŸã„ â€” want to"),
    "desire_hoshii": (r'ã»ã—ã„|æ¬²ã—ã„|ã»ã—ã‹ã£ãŸ', "ã»ã—ã„ â€” want"),
    "desire_tsumori": (r'ã¤ã‚‚ã‚Š[ã ã¯ã§ãª]', "ã¤ã‚‚ã‚Š â€” intend to"),
    "desire_you_to_suru": (r'ã‚ˆã†ã¨ã—[ãŸã¦]|ã‚ˆã†ã¨ã™ã‚‹', "ã‚ˆã†ã¨ã™ã‚‹ â€” try to"),
    "desire_ki_ni_naru": (r'æ°—ã«ãªã‚‹|æ°—ã«ãªã£ãŸ|æ°—ã«ãªã£ã¦', "æ°—ã«ãªã‚‹ â€” bothered by/curious"),

    # --- CONCESSION / CONTRAST (partially covered) ---
    "concession_noni": (r'ã®ã«[ã€‚ã€ã€]', "ã®ã« â€” even though/despite"),
    "concession_kuse_ni": (r'ãã›ã«|ç™–ã«', "ãã›ã« â€” even though (critical)"),
    "concession_toshitemo": (r'ã¨ã—ã¦ã‚‚|ã«ã—ã¦ã‚‚', "ã«ã—ã¦ã‚‚ â€” even if/granting that"),
    "concession_nagara_mo": (r'ãªãŒã‚‰ã‚‚', "ãªãŒã‚‰ã‚‚ â€” while/although"),

    # --- TOPIC / STRUCTURE PARTICLES ---
    "structure_koso": (r'ã“ã[ã€ã€‚ãŒã¯]', "ã“ã â€” precisely/it is X that"),
    "structure_bakari": (r'ã°ã‹ã‚Š[ã€ã€‚ã ã§]|ã°ã£ã‹ã‚Š', "ã°ã‹ã‚Š â€” only/nothing but"),
    "structure_shika_nai": (r'ã—ã‹ãªã„|ã—ã‹ãªã‹ã£ãŸ', "ã—ã‹ãªã„ â€” no choice but"),
    "structure_wake": (r'ã‚ã‘[ãŒã«ã§ã¯ã‚‚]|ã‚ã‘ãªã„', "ã‚ã‘ â€” reason/it means"),
    "structure_hazu": (r'ã¯ãš[ãŒã ã§ãªã®]', "ã¯ãš â€” should be/supposed to"),
    "structure_tokoro": (r'ã¨ã“ã‚ã [ã€‚ã£]|ã¨ã“ã‚ã ã£ãŸ', "ã¨ã“ã‚ã  â€” just about to/was about to"),

    # --- LIGHT NOVEL SPECIFIC ---
    "ln_muri": (r'ç„¡ç†[ã ã§ã¯ã€‚ï¼]|ãƒ ãƒª', "ç„¡ç† â€” impossible/can't"),
    "ln_uso": (r'å˜˜[ã ã§ã€‚ï¼ï¼Ÿ]|ã‚¦ã‚½', "å˜˜ â€” no way!/lie"),
    "ln_yabai": (r'ãƒ¤ãƒ[ã„ã‚¤]|ã‚„ã°ã„|ãƒ¤ãƒ[ã€‚ï¼]', "ãƒ¤ãƒã„ â€” oh no/amazing"),
    "ln_sugoi": (r'ã™ã”ã„|å‡„ã„|ã‚¹ã‚´ã‚¤|ã™ã’ãˆ|ã™ã’ã‡', "ã™ã”ã„ â€” amazing"),
    "ln_kawaii": (r'å¯æ„›ã„|ã‹ã‚ã„ã„|ã‚«ãƒ¯ã‚¤ã‚¤', "å¯æ„›ã„ â€” cute"),
    "ln_hazukashii": (r'æ¥ãšã‹ã—[ã„ã]|ã¯ãšã‹ã—[ã„ã]', "æ¥ãšã‹ã—ã„ â€” embarrassing"),
    "ln_mendokusai": (r'é¢å€’[ãã ]|ã‚ã‚“ã©ã†|ã‚ã‚“ã©ãã•ã„', "é¢å€’ â€” troublesome"),
    "ln_sonna": (r'ãã‚“ãª[ã€ã€‚ã“äº‹ï¼]', "ãã‚“ãª â€” such a/that kind of"),
    "ln_konna": (r'ã“ã‚“ãª[ã€ã€‚ã“äº‹ï¼]', "ã“ã‚“ãª â€” this kind of"),
    "ln_doushiyou": (r'ã©ã†ã—ã‚ˆã†|ã©ã†ã—ãŸã‚‰ã„ã„', "ã©ã†ã—ã‚ˆã† â€” what should I do"),
    "ln_dame": (r'ãƒ€ãƒ¡[ã ã§ã€‚ï¼]|ã ã‚[ã ã§ã€‚ï¼]|é§„ç›®', "ãƒ€ãƒ¡ â€” no good/not allowed"),
    "ln_honto": (r'æœ¬å½“[ã«ã ã§]|ã»ã‚“ã¨[ã†ã«ã ã§]|ãƒ›ãƒ³ãƒˆ', "æœ¬å½“ â€” really/truly"),
    "ln_zenzen": (r'å…¨ç„¶[ã€ã€‚]', "å…¨ç„¶ â€” not at all/totally"),
    "ln_suki": (r'å¥½ã[ã ã§ãªã€‚ï¼ã€]', "å¥½ã â€” like/love"),
    "ln_kirai": (r'å«Œã„[ã ã§ãªã€‚ï¼ã€]|ãã‚‰ã„', "å«Œã„ â€” dislike/hate"),
    "ln_ureshii": (r'å¬‰ã—[ã„ã]|ã†ã‚Œã—[ã„ã]', "å¬‰ã—ã„ â€” happy/glad"),
    "ln_kanashii": (r'æ‚²ã—[ã„ã]|ã‹ãªã—[ã„ã]', "æ‚²ã—ã„ â€” sad"),
    "ln_kowai": (r'æ€–[ã„ã]|ã“ã‚[ã„ã]', "æ€–ã„ â€” scary/afraid"),
    "ln_samishii": (r'å¯‚ã—[ã„ã]|ã•ã¿ã—[ã„ã]|ã•ã³ã—[ã„ã]', "å¯‚ã—ã„ â€” lonely"),
    "ln_tanoshii": (r'æ¥½ã—[ã„ã]|ãŸã®ã—[ã„ã]', "æ¥½ã—ã„ â€” fun/enjoyable"),
}

def main():
    input_dir = '/Users/damminhthang/Documents/WORK/AI_MODULES/MTL_STUDIO/pipeline/INPUT'
    epubs = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.epub')])
    
    print(f"Scanning for {len(CANDIDATE_PATTERNS)} candidate patterns across {len(epubs)} EPUBs...")
    print(f"This may take a few minutes...\n")
    
    pattern_counts = Counter()
    pattern_examples = defaultdict(list)
    total_lines = 0
    total_chars = 0
    epubs_processed = 0
    
    for i, epub_path in enumerate(epubs):
        if (i+1) % 20 == 0:
            print(f"  Processing {i+1}/{len(epubs)}...")
        try:
            text = extract_text_from_epub(epub_path)
            if not text:
                continue
            epubs_processed += 1
            lines = text.split('\n')
            total_lines += len(lines)
            total_chars += len(text)
            
            for line in lines:
                stripped = line.strip()
                if not stripped or len(stripped) < 5:
                    continue
                for pat_name, (regex, desc) in CANDIDATE_PATTERNS.items():
                    try:
                        matches = list(re.finditer(regex, stripped))
                        if matches:
                            pattern_counts[pat_name] += len(matches)
                            if len(pattern_examples[pat_name]) < 3:
                                pattern_examples[pat_name].append(stripped[:150])
                    except:
                        continue
        except:
            continue
    
    print(f"\n{'='*90}")
    print(f"âœ… Processed {epubs_processed}/{len(epubs)} EPUBs")
    print(f"ğŸ“Š Total lines: {total_lines:,}")
    print(f"ğŸ“Š Total chars: {total_chars:,}")
    print(f"{'='*90}")
    
    # === Group by category ===
    categories = defaultdict(list)
    for pat_name, count in pattern_counts.items():
        if pat_name.startswith('ln_'):
            category = 'LN_SPECIFIC'
        elif pat_name.startswith('ono_'):
            category = 'ONOMATOPOEIA'
        elif pat_name.startswith('mono_'):
            category = 'INNER_MONOLOGUE'
        elif pat_name.startswith('ending_'):
            category = 'SENTENCE_ENDINGS_NEW'
        elif pat_name.startswith('keigo_'):
            category = 'KEIGO'
        elif pat_name.startswith('quote_'):
            category = 'QUOTATION_HEARSAY'
        elif pat_name.startswith('hedging_'):
            category = 'HEDGING'
        elif pat_name.startswith('response_'):
            category = 'RESPONSE_PARTICLES'
        elif pat_name.startswith('transition_'):
            category = 'NATURAL_TRANSITIONS'
        elif pat_name.startswith('causative_') or pat_name.startswith('receiving_'):
            category = 'GIVING_RECEIVING'
        elif pat_name.startswith('desire_'):
            category = 'DESIRE_INTENTION'
        elif pat_name.startswith('concession_'):
            category = 'CONCESSION'
        elif pat_name.startswith('structure_'):
            category = 'STRUCTURE_PARTICLES'
        else:
            category = 'OTHER'
        categories[category].append((pat_name, count, CANDIDATE_PATTERNS[pat_name][1]))
    
    # Print results
    print(f"\n{'Category':<25} {'Pattern':<40} {'Freq':>8}  Description")
    print(f"{'-'*25} {'-'*40} {'-'*8}  {'-'*30}")
    
    grand_total = sum(pattern_counts.values())
    
    for cat in sorted(categories.keys(), key=lambda c: sum(x[1] for x in categories[c]), reverse=True):
        cat_total = sum(x[1] for x in categories[cat])
        pct = (cat_total / grand_total * 100) if grand_total else 0
        print(f"\nğŸ”µ {cat} (Total: {cat_total:,} | {pct:.1f}%)")
        for pat_name, count, desc in sorted(categories[cat], key=lambda x: x[1], reverse=True):
            if count > 0:
                print(f"  {'':.<23} {pat_name:<40} {count:>8,}  {desc}")
    
    # === Print examples for top patterns ===
    print(f"\n{'='*90}")
    print(f"EXAMPLE LINES FOR TOP-30 PATTERNS")
    print(f"{'='*90}")
    
    top30 = sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)[:30]
    for pat_name, count in top30:
        desc = CANDIDATE_PATTERNS[pat_name][1]
        print(f"\nğŸ“Œ {pat_name} ({count:,} hits) â€” {desc}")
        for ex in pattern_examples.get(pat_name, []):
            print(f"   â†’ {ex}")
    
    # === Save full results to JSON ===
    output = {
        "scan_stats": {
            "epubs_processed": epubs_processed,
            "total_lines": total_lines,
            "total_chars": total_chars,
            "patterns_scanned": len(CANDIDATE_PATTERNS)
        },
        "pattern_frequencies": dict(sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)),
        "pattern_examples": {k: v for k, v in pattern_examples.items()},
        "categories": {cat: {
            "total": sum(x[1] for x in pats),
            "patterns": {p[0]: {"count": p[1], "desc": p[2]} for p in pats}
        } for cat, pats in categories.items()}
    }
    
    out_path = '/Users/damminhthang/Documents/WORK/AI_MODULES/MTL_STUDIO/pipeline/scripts/corpus_scan_results.json'
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ Full results saved to: {out_path}")


if __name__ == '__main__':
    main()
