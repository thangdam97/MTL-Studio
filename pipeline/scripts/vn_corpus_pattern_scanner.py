#!/usr/bin/env python3
"""
VN Corpus Pattern Scanner
=========================
Scans 103 VN-translated chapters + 148 JP EPUBs to extract:
1. JP source pattern frequencies (per detector category)
2. Real VN translation examples for each pattern
3. VN-specific anti-AI-ism detection
4. VN particle density statistics

Output: corpus_scan_results_vn.json
"""

import json
import os
import re
import sys
from pathlib import Path
from collections import defaultdict, Counter
import zipfile
from html.parser import HTMLParser

# ============================================================================
# HTML STRIPPER (for EPUB extraction)
# ============================================================================

class HTMLStripper(HTMLParser):
    def __init__(self):
        super().__init__()
        self.text_parts = []
        self.skip_tags = {'script', 'style', 'head', 'rt', 'rp'}
        self._skip = False

    def handle_starttag(self, tag, attrs):
        if tag in self.skip_tags:
            self._skip = True
        if tag in ('br', 'p', 'div', 'h1', 'h2', 'h3', 'h4', 'li'):
            self.text_parts.append('\n')

    def handle_endtag(self, tag):
        if tag in self.skip_tags:
            self._skip = False

    def handle_data(self, data):
        if not self._skip:
            self.text_parts.append(data)

    def get_text(self):
        return ''.join(self.text_parts)


def strip_html(html_text):
    stripper = HTMLStripper()
    stripper.feed(html_text)
    return stripper.get_text()


# ============================================================================
# JP SOURCE PATTERN DEFINITIONS (matching grammar_pattern_detector.py)
# ============================================================================

JP_PATTERNS = {
    # --- 16 categories from grammar_pattern_detector.py ---
    "contrastive_comparison": [
        (r"ã‘ã©|ã‘ã‚Œã©|ã ã‘ã©", "kedo_contrast"),
        (r"ãŒã€|ã ãŒ", "ga_contrast"),
        (r"ã§ã‚‚", "demo_contrast"),
        (r"ã‚‚", "mo_also"),
        (r"ã®ã«", "noni_despite"),
    ],
    "dismissive_acknowledgment": [
        (r"ã¯ã¨ã‚‚ã‹ã|ã¯ã•ã¦ãŠã", "tomo_kaku"),
        (r"ã¯ã„ã„ã¨ã—ã¦", "wa_ii_toshite"),
        (r"ã¯ç½®ã„ã¦ãŠã„ã¦", "wa_oite"),
    ],
    "intensifiers": [
        (r"çµæ§‹|ã‘ã£ã“ã†", "kekkou"),
        (r"ã‹ãªã‚Š", "kanari"),
        (r"ã‚ã¡ã‚ƒ|ã‚ã£ã¡ã‚ƒ", "mecha"),
        (r"è¶…|ã¡ã‚‡ã†", "chou"),
        (r"ã™ã”ã|ã™ã”ã„|å‡„ã„", "sugoku"),
    ],
    "hedging": [
        (r"ã ã‚ã†|ã§ã—ã‚‡ã†", "darou"),
        (r"ã¨æ€ã†|ã¨æ€ã£ãŸ", "to_omou"),
        (r"ã‚ˆã†ã |ã‚ˆã†ãª", "you_da"),
        (r"ã¿ãŸã„", "mitai"),
        (r"ã‹ã‚‚ã—ã‚Œãªã„|ã‹ã‚‚", "kamoshirenai"),
        (r"ã‚‰ã—ã„", "rashii"),
        (r"æ°—ãŒã™ã‚‹", "ki_ga_suru"),
    ],
    "response_particles": [
        (r"ãã†(?:ã ã­|ã§ã™ã­)?", "sou_response"),
        (r"ã†ã‚“", "un_response"),
        (r"ã»ã‚‰", "hora"),
        (r"ã‚ã‚|ã‚ã", "aa_response"),
        (r"ãˆãˆ", "ee_response"),
        (r"ã­ãˆ|ã­ã‡", "nee_response"),
        (r"ãŠã„", "oi_response"),
        (r"ã¸ãˆ|ã¸ã‡", "hee_response"),
    ],
    "natural_transitions": [
        (r"ã ã‹ã‚‰", "dakara"),
        (r"ã§ã‚‚", "demo_transition"),
        (r"ã¨ã“ã‚ã§", "tokoro_de"),
        (r"ã ã£ã¦", "datte"),
        (r"ã—ã‹ã—", "shikashi"),
        (r"ã¤ã¾ã‚Š", "tsumari"),
        (r"ã¨ã«ã‹ã", "tonikaku"),
        (r"ãã‚Œã«", "soreni"),
        (r"ã•ã¦", "sate"),
        (r"ãã“ã§", "sokode"),
        (r"ãã‚Œã§", "sorede"),
    ],
    "sentence_endings": [
        (r"ãª[ãã‚]?(?:[ã€‚ï¼ï¼Ÿã€]|$)", "ending_na"),
        (r"ã®ã |ã‚“ã |ã‚“ã§ã™", "ending_noda"),
        (r"ã§ã—ã‚‡(?:ã†)?(?:[ã€‚ï¼ï¼Ÿã€]|$)", "ending_desho"),
        (r"ã£ã¦(?:[ã€‚ï¼ï¼Ÿã€]|$)", "ending_tte"),
        (r"ãž(?:[ã€‚ï¼ï¼Ÿã€]|$)", "ending_zo"),
        (r"ã•(?:[ã€‚ï¼ï¼Ÿã€]|$)", "ending_sa"),
        (r"ã‹ãª(?:[ã€‚ï¼ï¼Ÿã€]|$)", "ending_kana"),
        (r"ã‹ã—ã‚‰(?:[ã€‚ï¼ï¼Ÿã€]|$)", "ending_kashira"),
        (r"ã‚ã‚ˆ(?:[ã€‚ï¼ï¼Ÿã€]|$)", "ending_wa_yo"),
        (r"ã‚‚ã®?(?:[ã€‚ï¼ï¼Ÿã€]|$)", "ending_mono"),
        (r"ãªã®(?:[ã€‚ï¼ï¼Ÿã€]|$)", "ending_nano"),
    ],
    "emotional_nuance": [
        (r"ã¡ã‚‡ã£ã¨", "chotto"),
        (r"ãªã‚“ã ã‹|ä½•ã ã‹|ãªã‚“ã‹", "nandaka"),
        (r"ã‚„ã£ã±ã‚Š|ã‚„ã¯ã‚Š|ã‚„ã£ã±", "yappari"),
        (r"ã¾ã•ã‹", "masaka"),
        (r"ã•ã™ãŒ|æµçŸ³", "sasuga"),
        (r"ã¾ã‚|ã¾ã", "maa"),
        (r"åˆ¥ã«", "betsu_ni"),
        (r"ç¢ºã‹ã«|ãŸã—ã‹ã«", "tashika_ni"),
        (r"ã‚‚ã—ã‹ã—ã¦|ã²ã‚‡ã£ã¨ã—ã¦", "moshikashite"),
    ],
    "action_emphasis": [
        (r"ã¦ã—ã¾ã£ãŸ|ã¡ã‚ƒã£ãŸ", "te_shimatta"),
        (r"ã¦ã„ã‚‹|ã¦ã‚‹", "te_iru"),
        (r"ã¦ã¿ã‚‹|ã¦ã¿ãŸ", "te_miru"),
        (r"ã¦ãŠã|ã¨ã", "te_oku"),
        (r"ã¦ãã‚‹|ã¦ããŸ", "te_kuru"),
        (r"ã¦ã„ã|ã¦ã", "te_iku"),
        (r"ã¦ã‚ã‚‹", "te_aru"),
        (r"å§‹ã‚ã‚‹|å‡ºã™", "hajimeru_dasu"),
    ],
    "onomatopoeia": [
        (r"ãƒ‰ã‚­ãƒ‰ã‚­|ã©ãã©ã", "dokidoki"),
        (r"ãƒ‹ãƒ¤ãƒª|ã«ã‚„ã‚Š|ãƒ‹ãƒ¤", "niyari"),
        (r"ãƒãƒ©|ã¡ã‚‰", "chira"),
        (r"ã¯ã£ãã‚Š", "hakkiri"),
        (r"ã—ã£ã‹ã‚Š", "shikkari"),
        (r"ã‚­ãƒ©ã‚­ãƒ©|ãã‚‰ãã‚‰", "kirakira"),
        (r"ãƒ¯ã‚¯ãƒ¯ã‚¯|ã‚ãã‚ã", "wakuwaku"),
        (r"ã‚½ãƒ¯ã‚½ãƒ¯|ãã‚ãã‚", "sowasowa"),
        (r"ãƒ¢ã‚¸ãƒ¢ã‚¸|ã‚‚ã˜ã‚‚ã˜", "mojimuji"),
        (r"ã‚¬ãƒƒã‚«ãƒª|ãŒã£ã‹ã‚Š", "gakkari"),
        (r"ã‚¤ãƒ©ã‚¤ãƒ©|ã„ã‚‰ã„ã‚‰", "iraira"),
        (r"ãƒ•ãƒ¯ãƒ•ãƒ¯|ãµã‚ãµã‚", "fuwafuwa"),
        (r"ãƒœãƒ¼ãƒƒ|ã¼ãƒ¼ã£|ã¼ã‚„ã£", "boyatto"),
        (r"ãƒ”ãƒƒã‚¿ãƒª|ã´ã£ãŸã‚Š", "pittari"),
        (r"ã‚°ãƒƒã‚¹ãƒª|ãã£ã™ã‚Š", "gussuri"),
    ],
    "giving_receiving": [
        (r"ãã‚Œã‚‹|ãã‚ŒãŸ|ãã‚Œãªã„", "kureru"),
        (r"ã‚‚ã‚‰ã†|ã‚‚ã‚‰ã£ãŸ|ã‚‚ã‚‰ãˆ", "morau"),
        (r"ã‚ã’ã‚‹|ã‚ã’ãŸ", "ageru"),
        (r"ã„ãŸã ã|ã„ãŸã ã„", "itadaku"),
    ],
    "inner_monologue": [
        (r"æ€ã‚ãš", "omowazu"),
        (r"ãµã¨", "futo"),
        (r"ãªãœã‹|ä½•æ•…ã‹", "naze_ka"),
        (r"ã©ã†ã‚„ã‚‰", "douyara"),
        (r"ãã—ã¦", "soushite"),
        (r"ãã‚Œã§ã‚‚", "sore_demo"),
        (r"ã©ã†ã—ã¦ã‚‚", "doushitemo"),
        (r"ã¾ã•ã«", "masani"),
        (r"ã¤ã„", "tsui"),
        (r"ã„ã‹ã«ã‚‚", "ikanimo"),
    ],
    "quotation_hearsay": [
        (r"ã£ã¦è¨€ã†|ã£ã¦ã„ã†", "tte_iu"),
        (r"ã¨ã„ã†|ã¨è¨€ã†", "to_iu"),
        (r"ã¨ã‹ã„ã†|ã¨ã‹è¨€ã†", "toka_iu"),
        (r"ãã†ã (?!ã­)", "sou_da_hearsay"),
    ],
    "desire_intention": [
        (r"ãŸã„(?:[ã€‚ï¼ï¼Ÿã€\s]|$)", "tai_want"),
        (r"æ¬²ã—ã„|ã»ã—ã„", "hoshii"),
        (r"ã¤ã‚‚ã‚Š", "tsumori"),
        (r"ã‚ˆã†ã¨ã™ã‚‹", "you_to_suru"),
        (r"æ°—ã«ãªã‚‹", "ki_ni_naru"),
    ],
    "structure_particles": [
        (r"ã‚ã‘(?:ã |ãŒ|ã§|ã«|ã®|ã¯|ã˜ã‚ƒ|ãªã„)", "wake"),
        (r"ã¯ãš(?:ã |ãŒ|ã®|ãª)", "hazu"),
        (r"ã“ã", "koso"),
        (r"ã—ã‹ãªã„|ã»ã‹ãªã„", "shika_nai"),
        (r"ã°ã‹ã‚Š|ã°ã£ã‹ã‚Š", "bakari"),
        (r"ã¨ã“ã‚(?:ã |ã§|ãŒ|ã«)", "tokoro"),
    ],
    "concession_contrast": [
        (r"ã®ã«", "noni_concession"),
        (r"ã¨ã—ã¦ã‚‚|ã«ã—ã¦ã‚‚", "toshitemo"),
        (r"ãªãŒã‚‰ã‚‚|ãªãŒã‚‰", "nagara_mo"),
        (r"ãã›ã«|ãã›ã—ã¦", "kuse_ni"),
    ],
    # --- VN-SPECIFIC: additional patterns ---
    "keigo_register": [
        (r"ã§ã™(?:ãŒ|ã‹|ã­|ã‚ˆ)?", "desu"),
        (r"ã¾ã™(?:ãŒ|ã‹|ã­|ã‚ˆ)?", "masu"),
        (r"ãã ã•ã„|ä¸‹ã•ã„", "kudasai"),
        (r"ã„ãŸã [ãã]", "itadaku_keigo"),
        (r"ã”ã–ã„ã¾ã™", "gozaimasu"),
        (r"ã„ã‚‰ã£ã—ã‚ƒ[ã‚‹ã„]", "irassharu"),
        (r"ãŠã£ã—ã‚ƒ[ã‚‹ã„]", "ossharu"),
    ],
    "ln_specific_expressions": [
        (r"æœ¬å½“|ã»ã‚“ã¨", "honto"),
        (r"å¥½ã(?:ã |ãª|ã«|ã§)", "suki"),
        (r"ãã‚“ãª", "sonna"),
        (r"å¯æ„›ã„|ã‹ã‚ã„ã„", "kawaii"),
        (r"å¬‰ã—ã„|ã†ã‚Œã—ã„", "ureshii"),
        (r"ãƒ€ãƒ¡|ã ã‚|é§„ç›®", "dame"),
        (r"ã™ã”ã„|å‡„ã„", "sugoi"),
        (r"æ¥ãšã‹ã—ã„", "hazukashii"),
        (r"æ€–ã„|ã“ã‚ã„", "kowai"),
        (r"æ¥½ã—ã„|ãŸã®ã—ã„", "tanoshii"),
        (r"å¯‚ã—ã„|ã•ã¿ã—ã„|ã•ã³ã—ã„", "samishii"),
        (r"ã‚„ã°ã„|ãƒ¤ãƒã„", "yabai"),
        (r"å«Œã„|ãã‚‰ã„", "kirai"),
        (r"é¢å€’|ã‚ã‚“ã©ã†|ã‚ã‚“ã©ãã•ã„", "mendokusai"),
        (r"ã©ã†ã—ã‚ˆã†", "doushiyou"),
        (r"ç„¡ç†|ã‚€ã‚Š", "muri"),
        (r"å˜˜|ã†ã", "uso"),
        (r"ã“ã‚“ãª", "konna"),
        (r"å…¨ç„¶", "zenzen"),
    ],
}

# ============================================================================
# VN ANTI-AI-ISM DETECTION PATTERNS
# ============================================================================

VN_AI_ISM_PATTERNS = {
    "mot_cam_giac": (r"(?:má»™t )?cáº£m giÃ¡c (?:nhÆ° |lÃ  |báº¥t an|nháº¹ nhÃµm|cÄƒng tháº³ng|hoÃ i niá»‡m|tá»™i lá»—i)", "má»™t cáº£m giÃ¡c [X]"),
    "mot_cach": (r"má»™t cÃ¡ch [a-zA-ZÃ€-á»¹]+", "má»™t cÃ¡ch [adj]"),
    "viec_subject": (r"(?:^|\. )Viá»‡c [a-zA-ZÃ€-á»¹]+", "Viá»‡c [noun] as subject"),
    "su_nominalization": (r"(?:^|\. )Sá»± [a-zA-ZÃ€-á»¹]+", "Sá»± [noun] nominalization"),
    "dieu_overuse": (r"(?:^|\. )Äiá»u (?:Ä‘Ã³|nÃ y|áº¥y)", "Äiá»u [demonstrative]"),
    "khong_the_phu_nhan": (r"khÃ´ng thá»ƒ phá»§ nháº­n", "khÃ´ng thá»ƒ phá»§ nháº­n"),
    "nhan_ra_rang": (r"nháº­n ra ráº±ng", "nháº­n ra ráº±ng"),
    "tran_ngap": (r"trÃ n ngáº­p", "trÃ n ngáº­p"),
    "bao_phu": (r"bao phá»§", "bao phá»§"),
    "day_ap": (r"Ä‘áº§y áº¯p", "Ä‘áº§y áº¯p"),
}

# ============================================================================
# VN PARTICLE INVENTORY
# ============================================================================

VN_PARTICLES = {
    "conversational": ["mÃ ", "rá»“i", "thÃ¬", "nÃªn", "vÃ¬", "vá»›i láº¡i", "cho nÃªn", "tháº¿ nÃªn"],
    "emphasis": ["chá»©", "Ä‘áº¥y", "nhÃ©", "nha", "Ä‘Ã¢u", "cÆ¡", "kia", "mÃ "],
    "question": ["Ã ", "áº¡", "háº£", "nhá»‰", "chá»©", "sao"],
    "trailing": ["thÃ´i", "Ä‘i", "nÃ o", "váº­y", "tháº¿", "Ä‘Ã³"],
    "interjection": ["Ã´i", "trá»i", "chÃ ", "á»§a", "há»­", "á»ƒ", "Ã¡", "Æ¡"],
    "hedging": ["cháº¯c", "cÃ³ láº½", "hÃ¬nh nhÆ°", "dÆ°á»ng nhÆ°", "cháº¯c háº³n"],
    "intensifier": ["quÃ¡", "láº¯m", "tháº­t", "cá»±c ká»³", "vÃ´ cÃ¹ng", "siÃªu"],
}


# ============================================================================
# SCANNER FUNCTIONS
# ============================================================================

def extract_epub_text(epub_path):
    """Extract text from EPUB file."""
    lines = []
    try:
        with zipfile.ZipFile(epub_path, 'r') as z:
            for name in sorted(z.namelist()):
                if name.endswith(('.xhtml', '.html', '.htm')):
                    try:
                        html = z.read(name).decode('utf-8', errors='replace')
                        text = strip_html(html)
                        for line in text.split('\n'):
                            line = line.strip()
                            if line and len(line) > 2:
                                lines.append(line)
                    except Exception:
                        continue
    except Exception as e:
        print(f"  âš  Error reading {epub_path}: {e}")
    return lines


def scan_jp_patterns(lines):
    """Scan JP text for pattern frequencies."""
    results = defaultdict(lambda: defaultdict(int))
    examples = defaultdict(lambda: defaultdict(list))
    
    for line in lines:
        for category, patterns in JP_PATTERNS.items():
            for regex, pattern_id in patterns:
                if re.search(regex, line):
                    results[category][pattern_id] += 1
                    if len(examples[category][pattern_id]) < 3:
                        examples[category][pattern_id].append(line[:200])
    
    return results, examples


def scan_vn_patterns(lines):
    """Scan VN text for particle density and AI-isms."""
    total_words = 0
    particle_counts = defaultdict(int)
    ai_ism_counts = defaultdict(int)
    ai_ism_examples = defaultdict(list)
    
    for line in lines:
        words = line.split()
        total_words += len(words)
        
        # Count particles
        for category, particles in VN_PARTICLES.items():
            for particle in particles:
                count = len(re.findall(r'\b' + re.escape(particle) + r'\b', line, re.IGNORECASE))
                particle_counts[f"{category}:{particle}"] += count
        
        # Detect AI-isms
        for ai_id, (regex, desc) in VN_AI_ISM_PATTERNS.items():
            matches = re.findall(regex, line, re.IGNORECASE)
            if matches:
                ai_ism_counts[ai_id] += len(matches)
                if len(ai_ism_examples[ai_id]) < 3:
                    ai_ism_examples[ai_id].append(line[:200])
    
    return {
        "total_words": total_words,
        "particle_counts": dict(particle_counts),
        "ai_ism_counts": dict(ai_ism_counts),
        "ai_ism_examples": dict(ai_ism_examples),
    }


def scan_vn_chapters(work_dir):
    """Scan all VN chapter files in WORK directory."""
    vn_files = []
    for root, dirs, files in os.walk(work_dir):
        for f in files:
            if f.endswith("_VN.md") and "/VN/" in os.path.join(root, f):
                vn_files.append(os.path.join(root, f))
    
    print(f"\nðŸ“– Found {len(vn_files)} VN chapter files")
    
    all_lines = []
    for vf in vn_files:
        try:
            with open(vf, 'r', encoding='utf-8') as fh:
                lines = [l.strip() for l in fh.readlines() if l.strip() and len(l.strip()) > 2]
                all_lines.extend(lines)
        except Exception as e:
            print(f"  âš  Error reading {vf}: {e}")
    
    print(f"  Total VN lines: {len(all_lines)}")
    print(f"  Total VN chars: {sum(len(l) for l in all_lines)}")
    
    return all_lines, vn_files


def scan_jp_epubs(input_dir):
    """Scan all JP EPUB files in INPUT directory."""
    epub_files = sorted(Path(input_dir).glob("*.epub"))
    print(f"\nðŸ“š Found {len(epub_files)} JP EPUB files")
    
    all_lines = []
    for i, epub in enumerate(epub_files):
        if (i + 1) % 20 == 0:
            print(f"  Progress: {i + 1}/{len(epub_files)}")
        lines = extract_epub_text(str(epub))
        all_lines.extend(lines)
    
    print(f"  Total JP lines: {len(all_lines)}")
    print(f"  Total JP chars: {sum(len(l) for l in all_lines)}")
    
    return all_lines, epub_files


# ============================================================================
# JP-VN PAIRED EXAMPLE EXTRACTOR
# ============================================================================

def extract_paired_examples(work_dir):
    """
    Extract JP-VN paired examples from WORK directories.
    Looks for matching JP/VN chapter pairs.
    """
    paired_examples = defaultdict(list)
    pair_count = 0
    
    for project_dir in Path(work_dir).iterdir():
        if not project_dir.is_dir():
            continue
        
        vn_dir = project_dir / "VN"
        # Look for JP chapters in multiple possible locations
        jp_dirs = [project_dir / "JP", project_dir / "chapters", project_dir]
        
        if not vn_dir.exists():
            continue
        
        vn_chapters = sorted(vn_dir.glob("CHAPTER_*_VN.md"))
        
        for vn_chapter in vn_chapters:
            # Extract chapter number
            match = re.search(r'CHAPTER_(\d+)', vn_chapter.name)
            if not match:
                continue
            ch_num = match.group(1)
            
            # Find matching JP chapter
            jp_chapter = None
            for jp_dir in jp_dirs:
                candidates = [
                    jp_dir / f"CHAPTER_{ch_num}.md",
                    jp_dir / f"CHAPTER_{ch_num}_JP.md",
                    jp_dir / f"chapter_{ch_num}.md",
                ]
                for c in candidates:
                    if c.exists():
                        jp_chapter = c
                        break
                if jp_chapter:
                    break
            
            if not jp_chapter:
                continue
            
            try:
                with open(jp_chapter, 'r', encoding='utf-8') as f:
                    jp_lines = [l.strip() for l in f.readlines() if l.strip() and not l.startswith('#')]
                with open(vn_chapter, 'r', encoding='utf-8') as f:
                    vn_lines = [l.strip() for l in f.readlines() if l.strip() and not l.startswith('#')]
                
                # Align by paragraph index (rough alignment)
                min_len = min(len(jp_lines), len(vn_lines))
                for idx in range(min_len):
                    jp_line = jp_lines[idx]
                    vn_line = vn_lines[idx]
                    
                    # Check if JP line contains interesting patterns
                    for category, patterns in JP_PATTERNS.items():
                        for regex, pattern_id in patterns:
                            if re.search(regex, jp_line):
                                if len(paired_examples[pattern_id]) < 5:
                                    paired_examples[pattern_id].append({
                                        "jp": jp_line[:300],
                                        "vn": vn_line[:300],
                                        "source": f"{project_dir.name}/{vn_chapter.name}"
                                    })
                                    pair_count += 1
                
            except Exception:
                continue
    
    print(f"\nðŸ”— Extracted {pair_count} JP-VN paired examples across {len(paired_examples)} patterns")
    return dict(paired_examples)


# ============================================================================
# MAIN
# ============================================================================

def main():
    base_dir = Path(__file__).parent.parent
    input_dir = base_dir / "INPUT"
    work_dir = base_dir / "WORK"
    output_file = base_dir / "scripts" / "corpus_scan_results_vn.json"
    
    print("=" * 70)
    print("VN CORPUS PATTERN SCANNER")
    print("=" * 70)
    
    results = {
        "scan_stats": {},
        "jp_pattern_frequencies": {},
        "jp_pattern_examples": {},
        "jp_category_totals": {},
        "vn_analysis": {},
        "paired_examples": {},
    }
    
    # 1. Scan JP EPUBs
    print("\n" + "=" * 40)
    print("PHASE 1: JP Source Patterns (148 EPUBs)")
    print("=" * 40)
    jp_lines, epub_files = scan_jp_epubs(input_dir)
    
    jp_freqs, jp_examples = scan_jp_patterns(jp_lines)
    
    # Flatten frequencies
    flat_freqs = {}
    category_totals = {}
    for cat, patterns in jp_freqs.items():
        cat_total = 0
        for pid, count in patterns.items():
            flat_freqs[pid] = count
            cat_total += count
        category_totals[cat] = cat_total
    
    # Sort by frequency
    flat_freqs = dict(sorted(flat_freqs.items(), key=lambda x: x[1], reverse=True))
    category_totals = dict(sorted(category_totals.items(), key=lambda x: x[1], reverse=True))
    
    results["jp_pattern_frequencies"] = flat_freqs
    results["jp_category_totals"] = category_totals
    
    # Flatten examples
    flat_examples = {}
    for cat, patterns in jp_examples.items():
        for pid, exs in patterns.items():
            flat_examples[pid] = exs
    results["jp_pattern_examples"] = flat_examples
    
    print(f"\n  Total JP patterns found: {sum(flat_freqs.values())}")
    print(f"  Categories: {len(category_totals)}")
    print(f"\n  Top 15 categories by frequency:")
    for cat, total in list(category_totals.items())[:15]:
        print(f"    {cat}: {total:,}")
    
    # 2. Scan VN Chapters
    print("\n" + "=" * 40)
    print("PHASE 2: VN Output Analysis (103 chapters)")
    print("=" * 40)
    vn_lines, vn_files = scan_vn_chapters(work_dir)
    
    vn_analysis = scan_vn_patterns(vn_lines)
    results["vn_analysis"] = vn_analysis
    
    # VN stats
    total_words = vn_analysis["total_words"]
    print(f"\n  Total VN words: {total_words:,}")
    
    # Particle density
    total_particles = sum(vn_analysis["particle_counts"].values())
    if total_words > 0:
        density = (total_particles / total_words) * 1000
        print(f"  Particle density: {density:.1f} per 1000 words")
    
    # AI-ism report
    print(f"\n  AI-ism detections:")
    for ai_id, count in sorted(vn_analysis["ai_ism_counts"].items(), key=lambda x: x[1], reverse=True):
        desc = VN_AI_ISM_PATTERNS[ai_id][1]
        print(f"    {desc}: {count}")
    
    # Top particles
    print(f"\n  Top 20 VN particles:")
    sorted_particles = sorted(vn_analysis["particle_counts"].items(), key=lambda x: x[1], reverse=True)[:20]
    for particle, count in sorted_particles:
        print(f"    {particle}: {count}")
    
    # 3. Extract JP-VN pairs
    print("\n" + "=" * 40)
    print("PHASE 3: JP-VN Paired Examples")
    print("=" * 40)
    paired = extract_paired_examples(work_dir)
    results["paired_examples"] = paired
    
    # Stats
    results["scan_stats"] = {
        "jp_epubs_processed": len(epub_files),
        "jp_total_lines": len(jp_lines),
        "jp_total_chars": sum(len(l) for l in jp_lines),
        "vn_chapters_processed": len(vn_files),
        "vn_total_lines": len(vn_lines),
        "vn_total_chars": sum(len(l) for l in vn_lines),
        "vn_total_words": total_words,
        "jp_patterns_scanned": len(flat_freqs),
        "jp_categories": len(category_totals),
        "paired_examples_extracted": sum(len(v) for v in paired.values()),
    }
    
    # Save
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Results saved to {output_file}")
    print(f"   JP patterns: {len(flat_freqs)} unique, {sum(flat_freqs.values()):,} total hits")
    print(f"   VN words: {total_words:,}, particles: {total_particles:,}")
    print(f"   Paired examples: {sum(len(v) for v in paired.values())}")


if __name__ == "__main__":
    main()
