#!/usr/bin/env python3
"""
Japanese Dialogue & Grammar Pattern Analyzer

Scans EPUB corpus to extract linguistic patterns for Vietnamese grammar RAG improvement:
1. Japanese interjections (æ„Ÿå‹•è©ž) and their frequency
2. Sentence-ending particles (çµ‚åŠ©è©ž)
3. Honorific/register patterns (æ•¬èªž)
4. Emotional expressions
5. Onomatopoeia (æ“¬éŸ³èªž/æ“¬æ…‹èªž)
6. Dialogue-specific structures

Usage:
    python scripts/analyze_dialogue_patterns.py --scan      # Full corpus analysis
    python scripts/analyze_dialogue_patterns.py --interjections
    python scripts/analyze_dialogue_patterns.py --particles
    python scripts/analyze_dialogue_patterns.py --export    # Export to JSON

Author: MTL Studio
Date: 2025-01-31
"""

import sys
import json
import argparse
import re
import html
import zipfile
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Set

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent.parent))


# ============================================================
# Japanese Linguistic Patterns
# ============================================================

# Interjections (æ„Ÿå‹•è©ž) - Common in light novels
JAPANESE_INTERJECTIONS = {
    # Surprise/Shock
    r'ãˆã£': 'surprise_mild',
    r'ãˆãˆã£': 'surprise_strong',
    r'ãˆ[ãƒ¼ãƒ¼]+': 'surprise_prolonged',
    r'ã†ã‚[ã£]?': 'shock',
    r'ã†ã‚ãƒ¼': 'shock_prolonged',
    r'ã²ã£': 'fright',
    r'ãã‚ƒ[ã£ãƒ¼]?': 'fright_female',
    r'ãŽã‚ƒ[ã£ãƒ¼]?': 'fright_strong',
    r'ã‚ã£': 'startle',
    r'ã‚ã£': 'realization',
    
    # Agreement/Understanding
    r'ã†ã‚“': 'casual_yes',
    r'ã†ã†ã‚“': 'casual_no',
    r'ã¯ã„': 'formal_yes',
    r'ãˆãˆ': 'polite_yes',
    r'ãã†ã ã­': 'agreement',
    r'ãªã‚‹ã»ã©': 'understanding',
    r'ãã£ã‹': 'casual_understanding',
    
    # Hesitation/Thinking
    r'ãˆã£ã¨': 'hesitation',
    r'ãˆãƒ¼ã£ã¨': 'hesitation_long',
    r'ã‚ã®[ãƒ¼ãƒ¼]?': 'attention_getter',
    r'ãã®[ãƒ¼ãƒ¼]?': 'hesitation_polite',
    r'ãˆãˆã¨': 'thinking',
    r'ã†ãƒ¼ã‚“': 'pondering',
    r'ã‚“ãƒ¼': 'pondering_casual',
    
    # Emotional expressions
    r'ã‚„ã‚Œã‚„ã‚Œ': 'exasperation',
    r'ã‚ãƒ¼ã‚': 'disappointment',
    r'ã¯ã': 'sigh',
    r'ãµã…': 'relief_sigh',
    r'ã¡ã£': 'tch_annoyance',
    r'ãã£': 'frustration',
    r'ã‚€ã£': 'displeasure',
    r'ãµã‚“': 'dismissive',
    r'ã¸ã‡': 'impressed',
    r'ã»ã†': 'interest',
    
    # Calling/Attention
    r'ã­ã‡': 'attention_casual',
    r'ãªã': 'attention_male',
    r'ã¡ã‚‡ã£ã¨': 'hey_wait',
    r'ãŠã„': 'hey_rough',
    r'ã»ã‚‰': 'look_here',
    r'ã•ã': 'well_now',
    r'ã¾ã': 'well_soft',
    
    # Exclamations
    r'ãã': 'damn',
    r'ã¡ãã—ã‚‡ã†': 'damn_strong',
    r'ã—ã¾ã£ãŸ': 'oops',
    r'ã‚„ã£ãŸ': 'yay',
    r'ã‚ˆã—': 'alright',
    r'ã‚ˆã£ã—[ã‚ƒãƒ¼]+': 'yay_energetic',
    r'ã™ã’ãƒ¼': 'awesome_male',
    r'ã™ã”ã„': 'amazing',
    r'ã‚„ã°ã„': 'yabai',
    r'ãƒžã‚¸': 'seriously',
}

# Sentence-ending particles (çµ‚åŠ©è©ž)
SENTENCE_PARTICLES = {
    # Question markers
    r'ã‹[ï¼Ÿ?]?$': 'question_neutral',
    r'ã‹ãª[ï¼Ÿ?]?$': 'wondering',
    r'ã‹ã—ã‚‰[ï¼Ÿ?]?$': 'wondering_female',
    r'ã®[ï¼Ÿ?]$': 'question_soft',
    r'ã ã‚ã†[ï¼Ÿ?]?$': 'rhetorical_male',
    r'ã§ã—ã‚‡ã†[ï¼Ÿ?]?$': 'rhetorical_polite',
    
    # Emphasis
    r'ã‚ˆ[ï¼!]?$': 'emphasis',
    r'ã­[ï¼!]?$': 'seeking_agreement',
    r'ã‚ˆã­[ï¼!]?$': 'confirmation',
    r'ãž[ï¼!]?$': 'emphasis_male',
    r'ãœ[ï¼!]?$': 'casual_male',
    r'ã‚[ï¼!]?$': 'soft_female',
    r'ãª[ããƒ¼]?$': 'reflection',
    r'ã•$': 'casual_assertion',
    
    # Softening
    r'ã‘ã©[ã­]?$': 'trailing_but',
    r'ã‹ã‚‰[ã­]?$': 'because_trailing',
    r'ã—[ã­]?$': 'listing_reason',
    r'ã‚‚ã‚“$': 'childish_reason',
    r'ã®ã«$': 'disappointment',
}

# Honorific patterns
HONORIFIC_PATTERNS = {
    # Keigo levels
    r'ã§ã™[ã€‚ï¼Ÿ]': 'desu_polite',
    r'ã¾ã™[ã€‚ï¼Ÿ]': 'masu_polite',
    r'ã”ã–ã„ã¾ã™': 'gozaimasu_humble',
    r'ã„ã‚‰ã£ã—ã‚ƒ': 'irassharu_honorific',
    r'ãŠã£ã—ã‚ƒ': 'ossharu_honorific',
    r'ãã ã•': 'kudasai_polite_request',
    r'ã„ãŸã ': 'itadaku_humble',
    
    # Casual speech
    r'ã [ã€‚ï¼]': 'da_casual',
    r'ã˜ã‚ƒã‚“': 'jan_casual',
    r'ã£ã™': 'ssu_young_male',
    r'ã ã‚ˆ': 'dayo_casual_emphasis',
    r'ãªã®': 'nano_soft',
}

# Onomatopoeia categories
ONOMATOPOEIA = {
    # Emotions
    r'ãƒ‰ã‚­ãƒ‰ã‚­': 'heartbeat_nervous',
    r'ã‚ãã‚ã': 'excited',
    r'ã‚¤ãƒ©ã‚¤ãƒ©': 'irritated',
    r'ãƒ‹ã‚³ãƒ‹ã‚³': 'smiling',
    r'ãƒ‹ãƒ¤ãƒ‹ãƒ¤': 'grinning',
    r'ãƒ¡ã‚½ãƒ¡ã‚½': 'whimpering',
    r'ãã™ã‚“': 'sniffling',
    r'ãˆã¸ã¸': 'embarrassed_laugh',
    r'ã‚ã¯ã¯': 'laughing',
    r'ãµãµ': 'chuckling',
    r'ãã™ãã™': 'giggling',
    
    # Physical states
    r'ãã£ãŸã‚Š': 'exhausted',
    r'ã¼ãƒ¼ã£ã¨': 'dazed',
    r'ãã‚‡ã¨ã‚“': 'blank_look',
    r'ã˜ãƒ¼ã£ã¨': 'staring',
    r'ã¡ã‚‰ã¡ã‚‰': 'glancing',
    r'ã”ãã‚Š': 'gulping',
    
    # Actions/Sounds
    r'ã‚¬ãƒãƒ£': 'door_opening',
    r'ãƒã‚¿ãƒ³': 'door_slamming',
    r'ãƒ”ãƒ³ãƒãƒ³': 'doorbell',
    r'ãƒ–ãƒ«ãƒ–ãƒ«': 'vibrating/shivering',
    r'ã‚´ãƒ­ã‚´ãƒ­': 'rolling/thunder',
}

# Dialogue structure patterns
DIALOGUE_STRUCTURES = {
    # Incomplete sentences (trailing off)
    r'[ã€‚ã€]\.{2,3}': 'trailing_off',
    r'[ãƒ¼ãƒ¼]{2,}': 'prolonged_sound',
    r'ã£[ï¼!]': 'cut_off_emphatic',
    
    # Quoted speech patterns
    r'ã€Œ[^ã€]+ã€ã¨': 'quoted_with_to',
    r'ã€Ž[^ã€]+ã€': 'inner_thought',
    
    # Interruption patterns
    r'[ãƒ¼ãƒ¼]ã£': 'interrupted',
}


def extract_text_from_epub(epub_path: Path) -> str:
    """Extract text content from EPUB file."""
    text_content = []
    
    try:
        with zipfile.ZipFile(epub_path, 'r') as epub:
            for file_info in epub.filelist:
                filename = file_info.filename
                
                if not (filename.endswith('.xhtml') or filename.endswith('.html')):
                    continue
                
                if 'nav' in filename.lower() or 'toc' in filename.lower():
                    continue
                
                try:
                    content = epub.read(filename).decode('utf-8', errors='ignore')
                    text = re.sub(r'<[^>]+>', ' ', content)
                    text = html.unescape(text)
                    text_content.append(text)
                except:
                    continue
        
        return ' '.join(text_content)
    except Exception as e:
        print(f"Error reading {epub_path.name}: {e}")
        return ""


def extract_dialogue_lines(text: str) -> List[str]:
    """Extract dialogue lines (text within ã€Œã€brackets)."""
    dialogues = re.findall(r'ã€Œ([^ã€]+)ã€', text)
    return dialogues


def analyze_interjections(dialogues: List[str]) -> Counter:
    """Count interjection usage in dialogues."""
    counts = Counter()
    
    for dialogue in dialogues:
        for pattern, category in JAPANESE_INTERJECTIONS.items():
            matches = re.findall(pattern, dialogue)
            if matches:
                counts[category] += len(matches)
    
    return counts


def analyze_particles(dialogues: List[str]) -> Counter:
    """Analyze sentence-ending particles."""
    counts = Counter()
    
    for dialogue in dialogues:
        # Split into sentences
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', dialogue)
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            for pattern, category in SENTENCE_PARTICLES.items():
                if re.search(pattern, sentence):
                    counts[category] += 1
    
    return counts


def analyze_honorifics(text: str) -> Counter:
    """Analyze honorific/register patterns."""
    counts = Counter()
    
    for pattern, category in HONORIFIC_PATTERNS.items():
        matches = re.findall(pattern, text)
        counts[category] += len(matches)
    
    return counts


def analyze_onomatopoeia(text: str) -> Counter:
    """Count onomatopoeia usage."""
    counts = Counter()
    
    for pattern, category in ONOMATOPOEIA.items():
        matches = re.findall(pattern, text, re.IGNORECASE)
        counts[category] += len(matches)
    
    return counts


def analyze_corpus(input_dir: Path, limit: int = None) -> Dict:
    """
    Full corpus analysis.
    
    Returns aggregated statistics and patterns.
    """
    epub_files = sorted(input_dir.glob("*.epub"))
    if limit:
        epub_files = epub_files[:limit]
    
    print(f"Analyzing {len(epub_files)} EPUB files...")
    
    # Aggregated counters
    all_interjections = Counter()
    all_particles = Counter()
    all_honorifics = Counter()
    all_onomatopoeia = Counter()
    
    # Track per-book stats
    book_stats = []
    total_dialogues = 0
    total_chars = 0
    
    for i, epub_path in enumerate(epub_files):
        print(f"  [{i+1}/{len(epub_files)}] {epub_path.name[:50]}...")
        
        text = extract_text_from_epub(epub_path)
        if not text:
            continue
        
        dialogues = extract_dialogue_lines(text)
        
        # Analyze
        interjections = analyze_interjections(dialogues)
        particles = analyze_particles(dialogues)
        honorifics = analyze_honorifics(text)
        onomatopoeia = analyze_onomatopoeia(text)
        
        # Aggregate
        all_interjections.update(interjections)
        all_particles.update(particles)
        all_honorifics.update(honorifics)
        all_onomatopoeia.update(onomatopoeia)
        
        total_dialogues += len(dialogues)
        total_chars += len(text)
        
        book_stats.append({
            'name': epub_path.stem[:50],
            'dialogues': len(dialogues),
            'chars': len(text)
        })
    
    return {
        'summary': {
            'total_books': len(epub_files),
            'total_dialogues': total_dialogues,
            'total_characters': total_chars,
            'avg_dialogues_per_book': total_dialogues // len(epub_files) if epub_files else 0
        },
        'interjections': dict(all_interjections.most_common(50)),
        'particles': dict(all_particles.most_common(30)),
        'honorifics': dict(all_honorifics.most_common(20)),
        'onomatopoeia': dict(all_onomatopoeia.most_common(30)),
        'book_stats': book_stats[:10]  # Top 10 for brevity
    }


def generate_grammar_improvements(analysis: Dict) -> Dict:
    """
    Generate suggested improvements for vietnamese_grammar_rag.json
    based on corpus analysis.
    """
    improvements = {
        "version": "2.0",
        "description": "Corpus-derived improvements for Vietnamese Grammar RAG",
        "source": f"Analyzed from {analysis['summary']['total_books']} light novels",
        
        "new_interjection_mappings": {},
        "new_onomatopoeia_mappings": {},
        "register_frequency_data": {},
        "dialogue_density_baseline": {}
    }
    
    # Map interjections to Vietnamese
    INTERJECTION_VN_MAP = {
        'surprise_mild': ['Æ ', 'Háº£', 'á»¦a'],
        'surprise_strong': ['Æ !', 'CÃ¡i gÃ¬!', 'Sao!'],
        'shock': ['á»i', 'Trá»i', 'ChÃ '],
        'fright': ['Há»±', 'Ãi', 'Há»±!'],
        'fright_female': ['Kyaa', 'Ãi chÃ ', 'Ã”i'],
        'realization': ['Ã€', 'á»’', 'A'],
        'casual_yes': ['á»ª', 'á»œ', 'Um'],
        'casual_no': ['KhÃ´ng', 'á»ªm khÃ´ng', 'ÄÃ¢u cÃ³'],
        'formal_yes': ['VÃ¢ng', 'Dáº¡', 'VÃ¢ng áº¡'],
        'polite_yes': ['VÃ¢ng', 'Dáº¡ vÃ¢ng', 'á»ªm'],
        'hesitation': ['á»ªm', 'Ã€', 'á»œ'],
        'hesitation_long': ['á»ªm...', 'Ã€...', 'á»œ...'],
        'attention_getter': ['NÃ y', 'Æ  nÃ y', 'Ã€'],
        'thinking': ['á»ªm', 'Äá»ƒ xem', 'Hmm'],
        'pondering': ['Hmm', 'á»ªm...', 'Äá»ƒ nghÄ© xem'],
        'exasperation': ['Trá»i Æ¡i', 'ThÃ´i rá»“i', 'Má»‡t ghÃª'],
        'disappointment': ['Haiz', 'Thá»Ÿ dÃ i', 'Ã”i trá»i'],
        'sigh': ['HÃ ', 'PhÃ¹', 'Thá»Ÿ'],
        'relief_sigh': ['PhÃ¹', 'HÃº há»“n', 'May quÃ¡'],
        'tch_annoyance': ['Tch', 'XÃ¬', 'Há»«'],
        'frustration': ['Khh', 'Tá»©c', 'á»¨c'],
        'displeasure': ['Há»«', 'Hmph', 'XÃ¬'],
        'dismissive': ['Há»«', 'Há»©', 'XÃ¬'],
        'impressed': ['á»’', 'ChÃ ', 'Hay'],
        'interest': ['á»’', 'Hoh', 'ThÃº vá»‹'],
        'attention_casual': ['NÃ y', 'ÃŠ', 'NÃ¨'],
        'attention_male': ['NÃ y', 'ÃŠ', 'Æ i'],
        'hey_wait': ['Khoan', 'NÃ y', 'ÃŠ'],
        'hey_rough': ['ÃŠ', 'NÃ y', 'Æ i'],
        'look_here': ['NÃ y', 'Xem nÃ y', 'KÃ¬a'],
        'well_now': ['NÃ o', 'ThÃ´i nÃ o', 'Äi'],
        'well_soft': ['Ã™i', 'Ã”i', 'Ã€'],
        'damn': ['Cháº¿t tiá»‡t', 'Khá»‘n', 'Äá»“ khá»‘n'],
        'oops': ['Cháº¿t', 'ThÃ´i cháº¿t', 'Há»ng rá»“i'],
        'yay': ['Yeah', 'Hura', 'Tuyá»‡t'],
        'alright': ['Ok', 'ÄÆ°á»£c rá»“i', 'Tá»‘t'],
        'yay_energetic': ['Yess!', 'Tuyá»‡t vá»i!', 'Äá»‰nh!'],
        'awesome_male': ['Äá»‰nh', 'SiÃªu', 'BÃ¡'],
        'amazing': ['Tuyá»‡t', 'Äáº¹p', 'Hay'],
        'yabai': ['Cháº¿t', 'Xong rá»“i', 'Toang'],
        'seriously': ['Tháº­t Ã ', 'NghiÃªm tÃºc', 'Thiá»‡t háº£'],
    }
    
    # Only include patterns that appeared frequently in corpus
    for category, count in analysis['interjections'].items():
        if count >= 50 and category in INTERJECTION_VN_MAP:  # Threshold
            improvements['new_interjection_mappings'][category] = {
                'frequency': count,
                'vietnamese_options': INTERJECTION_VN_MAP[category],
                'confidence': 'high' if count > 200 else 'medium'
            }
    
    # Onomatopoeia mappings
    ONOMATOPOEIA_VN_MAP = {
        'heartbeat_nervous': ['tim Ä‘áº­p thÃ¬nh thá»‹ch', 'há»“i há»™p', 'Ä‘áº­p thÃ¬nh thÃ¬nh'],
        'excited': ['hÃ¡o há»©c', 'nÃ´n nao', 'pháº¥n khÃ­ch'],
        'irritated': ['bá»±c bá»™i', 'khÃ³ chá»‹u', 'cÃ¡u'],
        'smiling': ['tÆ°Æ¡i cÆ°á»i', 'ná»Ÿ ná»¥ cÆ°á»i', 'cÆ°á»i'],
        'grinning': ['cÆ°á»i toe toÃ©t', 'cÆ°á»i nham hiá»ƒm', 'nháº¿ch mÃ©p'],
        'whimpering': ['sá»¥t sá»‹t', 'thÃºt thÃ­t', 'ná»©c ná»Ÿ'],
        'embarrassed_laugh': ['cÆ°á»i ngÆ°á»£ng', 'hÃ¬ hÃ¬', 'he he'],
        'laughing': ['ha ha', 'cÆ°á»i lá»›n', 'cÆ°á»i'],
        'chuckling': ['khÃºc khÃ­ch', 'hÃ¬ hÃ¬', 'cÆ°á»i nháº¹'],
        'giggling': ['cÆ°á»i khÃºc khÃ­ch', 'cÆ°á»i hÃ­ hÃ­', 'cÆ°á»i'],
        'exhausted': ['kiá»‡t sá»©c', 'má»‡t láº£', 'rÃ£ rá»i'],
        'dazed': ['ngáº©n ngÆ¡', 'thá» tháº«n', 'lÆ¡ Ä‘á»…nh'],
        'blank_look': ['ngÆ¡ ngÃ¡c', 'trá»‘ng rá»—ng', 'khÃ´ng hiá»ƒu'],
        'staring': ['nhÃ¬n cháº±m cháº±m', 'chÄƒm chÃº', 'Ä‘Äƒm Ä‘Äƒm'],
        'glancing': ['liáº¿c', 'Ä‘Æ°a máº¯t', 'nhÃ¬n lÃ©n'],
        'gulping': ['nuá»‘t nÆ°á»›c bá»t', 'á»±c', 'nuá»‘t khan'],
    }
    
    for category, count in analysis['onomatopoeia'].items():
        if count >= 20 and category in ONOMATOPOEIA_VN_MAP:
            improvements['new_onomatopoeia_mappings'][category] = {
                'frequency': count,
                'vietnamese_options': ONOMATOPOEIA_VN_MAP[category]
            }
    
    # Register frequency data
    improvements['register_frequency_data'] = {
        'polite_vs_casual_ratio': (
            analysis['honorifics'].get('desu_polite', 0) + 
            analysis['honorifics'].get('masu_polite', 0)
        ) / max(1, analysis['honorifics'].get('da_casual', 0) + 
                analysis['honorifics'].get('dayo_casual_emphasis', 0)),
        'note': 'Light novels typically have 60-70% casual speech'
    }
    
    # Dialogue density baseline
    improvements['dialogue_density_baseline'] = {
        'avg_dialogues_per_chapter': analysis['summary']['avg_dialogues_per_book'] // 10,
        'expected_particle_rate': '80%+ of dialogue lines should have particles'
    }
    
    return improvements


def print_analysis(analysis: Dict):
    """Pretty print analysis results."""
    print("\n" + "="*70)
    print("CORPUS ANALYSIS RESULTS")
    print("="*70)
    
    summary = analysis['summary']
    print(f"\nðŸ“š Corpus Summary:")
    print(f"   Total books analyzed: {summary['total_books']}")
    print(f"   Total dialogue lines: {summary['total_dialogues']:,}")
    print(f"   Total characters: {summary['total_characters']:,}")
    print(f"   Avg dialogues/book: {summary['avg_dialogues_per_book']}")
    
    print(f"\nðŸŽ­ Top Interjections (by frequency):")
    for cat, count in list(analysis['interjections'].items())[:15]:
        print(f"   {cat}: {count:,}")
    
    print(f"\nðŸ’¬ Top Sentence Particles:")
    for cat, count in list(analysis['particles'].items())[:10]:
        print(f"   {cat}: {count:,}")
    
    print(f"\nðŸŽ© Honorific Patterns:")
    for cat, count in list(analysis['honorifics'].items())[:10]:
        print(f"   {cat}: {count:,}")
    
    print(f"\nðŸ”Š Top Onomatopoeia:")
    for cat, count in list(analysis['onomatopoeia'].items())[:10]:
        print(f"   {cat}: {count:,}")


def main():
    parser = argparse.ArgumentParser(description="Analyze Japanese dialogue patterns")
    parser.add_argument('--scan', action='store_true', help='Full corpus scan')
    parser.add_argument('--limit', type=int, default=None, help='Limit books to analyze')
    parser.add_argument('--export', action='store_true', help='Export improvements JSON')
    parser.add_argument('--output', type=str, default='grammar_improvements.json')
    
    args = parser.parse_args()
    
    input_dir = Path(__file__).parent.parent / "INPUT"
    
    if not input_dir.exists():
        print(f"Error: INPUT directory not found at {input_dir}")
        return 1
    
    if args.scan or args.export:
        analysis = analyze_corpus(input_dir, limit=args.limit)
        print_analysis(analysis)
        
        if args.export:
            improvements = generate_grammar_improvements(analysis)
            
            output_path = Path(__file__).parent.parent / "VN" / args.output
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(improvements, f, ensure_ascii=False, indent=2)
            
            print(f"\nâœ… Exported improvements to: {output_path}")
            
            # Also save raw analysis
            analysis_path = Path(__file__).parent.parent / "VN" / "corpus_analysis.json"
            with open(analysis_path, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Saved raw analysis to: {analysis_path}")
    else:
        parser.print_help()
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
