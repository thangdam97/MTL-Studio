"""
VN Prose Refiner - Post-processor for Vietnamese AI-ism elimination
Transforms AI-sounding Vietnamese into natural prose

Based on vietnamese_grammar_rag.json patterns:
1. "má»™t cÃ¡ch [adj]" â†’ direct adverb or vivid verb
2. "má»™t cáº£m giÃ¡c" â†’ direct emotion
3. "sá»± [noun] cá»§a" â†’ use verb form
4. Missing particles â†’ add natural Vietnamese particles

Author: MTL Studio
Version: 1.0
"""

import re
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import json


@dataclass
class Refinement:
    """Single refinement applied"""
    pattern: str
    original: str
    replacement: str
    line_num: int
    category: str


class VNProseRefiner:
    """
    Post-processor to eliminate AI-isms from Vietnamese translations
    """
    
    def __init__(self):
        """Initialize with Vietnamese AI-ism patterns and their fixes"""
        
        # "má»™t cÃ¡ch [adj]" replacements - most common AI-ism
        self.mot_cach_map = {
            # Food/eating context
            "má»™t cÃ¡ch thanh Ä‘áº¡m": "thanh Ä‘áº¡m",
            "má»™t cÃ¡ch tÄ©nh láº·ng": "láº·ng láº½",
            "má»™t cÃ¡ch tá»« tá»‘n": "tá»« tá»‘n",
            "má»™t cÃ¡ch má»m máº¡i": "má»m máº¡i",
            "má»™t cÃ¡ch ngon lÃ nh": "ngon lÃ nh",
            "má»™t cÃ¡ch Ä‘áº§y thÆ°á»Ÿng thá»©c": "Ä‘áº§y thÆ°á»Ÿng thá»©c",
            "má»™t cÃ¡ch say mÃª": "say mÃª",
            "má»™t cÃ¡ch Ä‘áº§y thá»a mÃ£n": "thá»a mÃ£n",
            "má»™t cÃ¡ch trá»n váº¹n": "trá»n váº¹n",
            
            # Emotional/behavioral context
            "má»™t cÃ¡ch nghiÃªm tÃºc": "nghiÃªm tÃºc",
            "má»™t cÃ¡ch xuáº¥t sáº¯c": "xuáº¥t sáº¯c",
            "má»™t cÃ¡ch Ä‘áº§y thÃ¡ch thá»©c": "Ä‘áº§y thÃ¡ch thá»©c",
            "má»™t cÃ¡ch bÃ­ áº©n": "bÃ­ áº©n",
            "má»™t cÃ¡ch Ä‘á»™t ngá»™t": "Ä‘á»™t ngá»™t",
            "má»™t cÃ¡ch cháº­m rÃ£i": "cháº­m rÃ£i",
            "má»™t cÃ¡ch vá»™i vÃ ng": "vá»™i vÃ£",
            "má»™t cÃ¡ch cáº©n tháº­n": "cáº©n tháº­n",
            "má»™t cÃ¡ch tá»± nhiÃªn": "tá»± nhiÃªn",
            "má»™t cÃ¡ch ká»³ láº¡": "ká»³ láº¡",
            "má»™t cÃ¡ch Ä‘Ã¡ng ngáº¡c nhiÃªn": "Ä‘Ã¡ng ngáº¡c nhiÃªn",
            "má»™t cÃ¡ch ná»­a vá»i": "ná»­a vá»i",
            "má»™t cÃ¡ch trÃ¬u máº¿n": "trÃ¬u máº¿n",
            "má»™t cÃ¡ch dá»‹u dÃ ng": "dá»‹u dÃ ng",
            "má»™t cÃ¡ch máº¡nh máº½": "máº¡nh máº½",
            "má»™t cÃ¡ch nháº¹ nhÃ ng": "nháº¹ nhÃ ng",
            "má»™t cÃ¡ch hoÃ n háº£o": "hoÃ n háº£o",
            "má»™t cÃ¡ch tuyá»‡t vá»i": "tuyá»‡t vá»i",
            "má»™t cÃ¡ch Ä‘áº¹p Ä‘áº½": "Ä‘áº¹p Ä‘áº½",
            "má»™t cÃ¡ch láº·ng láº½": "láº·ng láº½",
            "má»™t cÃ¡ch Ã¢m tháº§m": "Ã¢m tháº§m",
            "má»™t cÃ¡ch rÃµ rÃ ng": "rÃµ rÃ ng",
            "má»™t cÃ¡ch mÆ¡ há»“": "mÆ¡ há»“",
            "má»™t cÃ¡ch chÃ¢n thÃ nh": "chÃ¢n thÃ nh",
            "má»™t cÃ¡ch trung thá»±c": "trung thá»±c",
            "má»™t cÃ¡ch khÃ©o lÃ©o": "khÃ©o lÃ©o",
            "má»™t cÃ¡ch tinh táº¿": "tinh táº¿",
            "má»™t cÃ¡ch sÃ¢u sáº¯c": "sÃ¢u sáº¯c",
            "má»™t cÃ¡ch toÃ n diá»‡n": "toÃ n diá»‡n",
            "má»™t cÃ¡ch triá»‡t Ä‘á»ƒ": "triá»‡t Ä‘á»ƒ",
            "má»™t cÃ¡ch kiÃªn quyáº¿t": "kiÃªn quyáº¿t",
            "má»™t cÃ¡ch dá»©t khoÃ¡t": "dá»©t khoÃ¡t",
            "má»™t cÃ¡ch tÃ¡o báº¡o": "tÃ¡o báº¡o",
            "má»™t cÃ¡ch liá»u lÄ©nh": "liá»u lÄ©nh",
            "má»™t cÃ¡ch khÃ¡c thÆ°á»ng": "khÃ¡c thÆ°á»ng",
            "má»™t cÃ¡ch phi thÆ°á»ng": "phi thÆ°á»ng",
            "má»™t cÃ¡ch báº¥t ngá»": "báº¥t ngá»",
            "má»™t cÃ¡ch Ä‘Ã¡ng sá»£": "Ä‘Ã¡ng sá»£",
            "má»™t cÃ¡ch Ä‘Ã¡ng yÃªu": "Ä‘Ã¡ng yÃªu",
            
            # From 2218 remaining patterns (round 2)
            "má»™t cÃ¡ch láº¡ thÆ°á»ng": "láº¡ thÆ°á»ng",
            "má»™t cÃ¡ch ngoáº¡n má»¥c": "ngoáº¡n má»¥c",
            "má»™t cÃ¡ch Ä‘Ãºng Ä‘áº¯n": "Ä‘Ãºng Ä‘áº¯n",
            "má»™t cÃ¡ch vÃ´ Ã­ch": "vÃ´ Ã­ch",
            "má»™t cÃ¡ch thuáº§n tÃºy": "thuáº§n tÃºy",
            "má»™t cÃ¡ch tháº­n trá»ng": "tháº­n trá»ng",
            "má»™t cÃ¡ch tháº£n nhiÃªn": "tháº£n nhiÃªn",
            "má»™t cÃ¡ch tao nhÃ£": "tao nhÃ£",
            "má»™t cÃ¡ch suÃ´n sáº»": "suÃ´n sáº»",
            "má»™t cÃ¡ch say sÆ°a": "say sÆ°a",
            "má»™t cÃ¡ch nÃ£o ná»": "nÃ£o ná»",
            "má»™t cÃ¡ch mÃ£n nguyá»‡n": "mÃ£n nguyá»‡n",
            "má»™t cÃ¡ch lá»‹ch sá»±": "lá»‹ch sá»±",
            "má»™t cÃ¡ch khá»e máº¡nh": "khá»e máº¡nh",
            "má»™t cÃ¡ch hÃ o phÃ³ng": "hÃ o phÃ³ng",
            "má»™t cÃ¡ch gÆ°á»£ng gáº¡o": "gÆ°á»£ng gáº¡o",
            "má»™t cÃ¡ch dá»¯ dá»™i": "dá»¯ dá»™i",
            "má»™t cÃ¡ch dá»… hiá»ƒu": "dá»… hiá»ƒu",
            "má»™t cÃ¡ch dÃ¨ dáº·t": "dÃ¨ dáº·t",
            "má»™t cÃ¡ch Ä‘áº§y phÃ´ trÆ°Æ¡ng": "phÃ´ trÆ°Æ¡ng",
            "má»™t cÃ¡ch Ä‘áº§y mong Ä‘á»£i": "Ä‘áº§y mong Ä‘á»£i",
            "má»™t cÃ¡ch Ä‘áº§y khÃ­ tháº¿": "Ä‘áº§y khÃ­ tháº¿",
            "má»™t cÃ¡ch Ä‘Ã¡ng ngá»": "Ä‘Ã¡ng ngá»",
            "má»™t cÃ¡ch cá»©ng Ä‘á»": "cá»©ng Ä‘á»",
            "má»™t cÃ¡ch chÄƒm chÃº": "chÄƒm chÃº",
            "má»™t cÃ¡ch bÃ¬nh tháº£n": "bÃ¬nh tháº£n",
            "má»™t cÃ¡ch báº£n nÄƒng": "theo báº£n nÄƒng",
        }
        
        # "má»™t cáº£m giÃ¡c" replacements
        self.cam_giac_map = {
            "má»™t cáº£m giÃ¡c báº¥t an": "sá»± báº¥t an",
            "má»™t cáº£m giÃ¡c nháº¹ nhÃµm": "sá»± nháº¹ nhÃµm",
            "má»™t cáº£m giÃ¡c cÄƒng tháº³ng": "sá»± cÄƒng tháº³ng",
            "má»™t cáº£m giÃ¡c hoÃ i niá»‡m": "ná»—i hoÃ i niá»‡m",
            "má»™t cáº£m giÃ¡c tá»™i lá»—i": "cáº£m giÃ¡c tá»™i lá»—i",
            "má»™t cáº£m giÃ¡c ká»³ láº¡": "cáº£m giÃ¡c ká»³ láº¡",
            "má»™t cáº£m giÃ¡c quen thuá»™c": "cáº£m giÃ¡c quen thuá»™c",
            "má»™t cáº£m giÃ¡c áº¥m Ã¡p": "sá»± áº¥m Ã¡p",
            "má»™t cáº£m giÃ¡c háº¡nh phÃºc": "niá»m háº¡nh phÃºc",
            "má»™t cáº£m giÃ¡c cÃ´ Ä‘Æ¡n": "ná»—i cÃ´ Ä‘Æ¡n",
            "má»™t cáº£m giÃ¡c buá»“n bÃ£": "ná»—i buá»“n",
        }
        
        # Context-aware sentence patterns
        self.sentence_patterns = [
            # "tÃ´i cÃ³ cáº£m giÃ¡c nhÆ°" patterns
            (r"[Tt]Ã´i cÃ³ cáº£m giÃ¡c nhÆ°", "Tá»±a nhÆ°"),
            (r"[Cc]Ã³ cáº£m giÃ¡c nhÆ°", "NhÆ° thá»ƒ"),
            
            # "sá»± xuáº¥t hiá»‡n cá»§a X" â†’ "X xuáº¥t hiá»‡n"
            (r"[Ss]á»± xuáº¥t hiá»‡n cá»§a ([a-zA-ZÃ€-á»¹\s]+) khiáº¿n", r"\1 xuáº¥t hiá»‡n khiáº¿n"),
            (r"[Ss]á»± ra Ä‘i cá»§a ([a-zA-ZÃ€-á»¹\s]+) khiáº¿n", r"\1 ra Ä‘i khiáº¿n"),
            (r"[Ss]á»± thay Ä‘á»•i cá»§a ([a-zA-ZÃ€-á»¹\s]+)", r"\1 thay Ä‘á»•i"),
            
            # "Viá»‡c X lÃ  Y" â†’ "X lÃ  Y"
            (r"^Viá»‡c ([a-zA-ZÃ€-á»¹\s]+) lÃ ", r"\1 lÃ "),
            (r"^Viá»‡c nÃ y giÃºp", "Äiá»u nÃ y giÃºp"),
        ]
        
        # Statistics
        self.stats = {
            "total_refinements": 0,
            "by_category": {},
            "by_chapter": {}
        }
    
    def refine_text(self, text: str, chapter_id: str = "00") -> Tuple[str, List[Refinement]]:
        """
        Refine Vietnamese text to eliminate AI-isms
        
        Args:
            text: Vietnamese text content
            chapter_id: Chapter identifier for tracking
            
        Returns:
            Tuple of (refined_text, list_of_refinements)
        """
        refinements = []
        lines = text.split('\n')
        refined_lines = []
        
        for line_num, line in enumerate(lines, 1):
            original_line = line
            
            # Apply "má»™t cÃ¡ch" fixes
            for pattern, replacement in self.mot_cach_map.items():
                if pattern in line:
                    line = line.replace(pattern, replacement)
                    refinements.append(Refinement(
                        pattern=pattern,
                        original=original_line,
                        replacement=replacement,
                        line_num=line_num,
                        category="mot-cach"
                    ))
            
            # Apply "má»™t cáº£m giÃ¡c" fixes
            for pattern, replacement in self.cam_giac_map.items():
                if pattern in line:
                    line = line.replace(pattern, replacement)
                    refinements.append(Refinement(
                        pattern=pattern,
                        original=original_line,
                        replacement=replacement,
                        line_num=line_num,
                        category="mot-cam-giac"
                    ))
            
            # Apply sentence-level patterns (regex)
            for pattern, replacement in self.sentence_patterns:
                if re.search(pattern, line):
                    new_line = re.sub(pattern, replacement, line)
                    if new_line != line:
                        refinements.append(Refinement(
                            pattern=pattern,
                            original=line,
                            replacement=new_line,
                            line_num=line_num,
                            category="sentence-pattern"
                        ))
                        line = new_line
            
            refined_lines.append(line)
        
        # Update stats
        self.stats["total_refinements"] += len(refinements)
        self.stats["by_chapter"][chapter_id] = len(refinements)
        for r in refinements:
            self.stats["by_category"][r.category] = self.stats["by_category"].get(r.category, 0) + 1
        
        return '\n'.join(refined_lines), refinements
    
    def refine_chapter_file(self, file_path: str, dry_run: bool = False) -> Dict:
        """
        Refine a single chapter file
        
        Args:
            file_path: Path to VN chapter file
            dry_run: If True, don't write changes
            
        Returns:
            Dict with refinement results
        """
        path = Path(file_path)
        if not path.exists():
            return {"error": f"File not found: {file_path}"}
        
        # Extract chapter ID
        match = re.search(r'CHAPTER_(\d+)', path.name)
        chapter_id = match.group(1) if match else "00"
        
        # Read content
        content = path.read_text(encoding='utf-8')
        
        # Refine
        refined_content, refinements = self.refine_text(content, chapter_id)
        
        # Write if not dry run and changes were made
        if not dry_run and refinements:
            path.write_text(refined_content, encoding='utf-8')
        
        return {
            "file": path.name,
            "chapter": chapter_id,
            "refinements_count": len(refinements),
            "refinements": [
                {
                    "line": r.line_num,
                    "category": r.category,
                    "pattern": r.pattern,
                    "fixed": r.replacement
                }
                for r in refinements
            ],
            "dry_run": dry_run
        }
    
    def refine_volume(self, volume_path: str, dry_run: bool = False) -> Dict:
        """
        Refine all VN chapters in a volume
        
        Args:
            volume_path: Path to volume directory
            dry_run: If True, don't write changes
            
        Returns:
            Dict with all refinement results
        """
        path = Path(volume_path)
        vn_dir = path / "VN"
        
        if not vn_dir.exists():
            return {"error": f"VN directory not found: {vn_dir}"}
        
        results = {
            "volume": path.name,
            "dry_run": dry_run,
            "chapters": [],
            "summary": {
                "total_files": 0,
                "total_refinements": 0,
                "by_category": {}
            }
        }
        
        # Reset stats
        self.stats = {
            "total_refinements": 0,
            "by_category": {},
            "by_chapter": {}
        }
        
        # Process each chapter
        for vn_file in sorted(vn_dir.glob("CHAPTER_*_VN.md")):
            result = self.refine_chapter_file(str(vn_file), dry_run)
            results["chapters"].append(result)
            results["summary"]["total_files"] += 1
            results["summary"]["total_refinements"] += result.get("refinements_count", 0)
        
        # Add category breakdown
        results["summary"]["by_category"] = self.stats["by_category"]
        
        return results


def run_vn_prose_refiner(volume_id: str, dry_run: bool = False):
    """
    CLI entry point for VN Prose Refiner
    
    Args:
        volume_id: Volume to refine (e.g., "2218")
        dry_run: If True, show changes without applying
    """
    # Find volume path
    work_dir = Path(__file__).parent.parent / "WORK"
    
    volume_path = None
    for folder in work_dir.iterdir():
        if folder.is_dir() and volume_id in folder.name:
            volume_path = folder
            break
    
    if not volume_path:
        print(f"âŒ Volume {volume_id} not found in WORK directory")
        return
    
    print(f"{'ðŸ” DRY RUN:' if dry_run else 'âœï¸ REFINING:'} Vietnamese prose for volume {volume_id}")
    print(f"   Path: {volume_path.name}")
    print()
    
    # Create refiner and run
    refiner = VNProseRefiner()
    results = refiner.refine_volume(str(volume_path), dry_run)
    
    # Print results
    print("=" * 60)
    print("REFINEMENT SUMMARY")
    print("=" * 60)
    print(f"Files processed: {results['summary']['total_files']}")
    print(f"Total refinements: {results['summary']['total_refinements']}")
    print()
    
    print("By Category:")
    for cat, count in sorted(results['summary']['by_category'].items(), key=lambda x: -x[1]):
        print(f"  - {cat}: {count}")
    print()
    
    print("By Chapter:")
    for chapter in results['chapters']:
        if chapter.get('refinements_count', 0) > 0:
            print(f"  - Chapter {chapter['chapter']}: {chapter['refinements_count']} refinements")
    
    # Save detailed results
    if not dry_run:
        results_path = volume_path / "audits" / "prose_refinements.json"
        results_path.parent.mkdir(exist_ok=True)
        results_path.write_text(json.dumps(results, indent=2, ensure_ascii=False), encoding='utf-8')
        print(f"\nðŸ“„ Details saved to: {results_path}")
    
    return results


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python vn_prose_refiner.py <volume_id> [--dry-run]")
        print("Example: python vn_prose_refiner.py 2218 --dry-run")
        sys.exit(1)
    
    volume_id = sys.argv[1]
    dry_run = "--dry-run" in sys.argv
    
    run_vn_prose_refiner(volume_id, dry_run)
